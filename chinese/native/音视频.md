# AAC详解
##  ADTS  (Audio Data Transport Stream)
```text
有的时候当你编码AAC裸流的时候，会遇到写出来的AAC文件并不能播放，
很大的可能就是AAC文件的每一帧里缺少了ADTS头信息文件的包装拼接。
只需要加入头文件ADTS即可。
```
 
## aac数据流格式  
![](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/201808141719_osChina_AAC数据流格式.png)
```text
AAC ES : Elementary Streams (原始流)
```

 
##  ADTS Heads
```text
ADTS的头信息都是7个字节，分为2部分：
adts_fixed_header(); // 28bit
adts_variable_header(); // 28bit
```

adts_fixed_header的字段|占用bit数|意义
-|-|-
syncword|12|同步头 总是0xFFF 。代表着一个ADTS帧的开始
ID|1|MPEG Version <br>0 代表 MPEG-4, 1代表 MPEG-2
Layer|2|always: '00'
protection_absent |1|Warning, 1:没有CRC , 0:有CRC
profile|2|使用的AAC级别 <br> 0: main profile  <br>1:low complexity profile (LC)  <br>2: scalable sampling rate profile (SSR)  <br>3:保留字段 reserved
sampling_frequency_index|4| 使用的采样率下标<br>// 下标对应 的采样率 <br>0: 96000 Hz <br>1: 88200 Hz <br>2: 64000 Hz <br>3: 48000 Hz <br>4: 44100 Hz <br>5: 32000 Hz <br>6: 24000 Hz <br>7: 22050 Hz <br>8: 16000 Hz <br>9: 12000 Hz <br>10: 11025 Hz <br>11: 8000 Hz <br>12: 7350 Hz <br>13: Reserved <br>14: Reserved <br>15: frequency is written explictly
private_bit|1|固定为0
channel_configuration|3|声道数<br>0: Defined in AOT Specifc Config <br>1: 1 声道: front-center <br>2: 2 声道: front-left, front-right <br>3: 3 声道: front-center, front-left, front-right <br>4: 4 声道: front-center, front-left, front-right, <br> back-center <br>5: 5 声道: front-center, front-left, front-right,  <br>back-left, back-right <br>6: 6 声道: front-center, front-left, front-right,  <br>back-left, back-right, LFE-channel <br>7: 8 声道: front-center, front-left, front-right,  <br>side-left, side-right, back-left,  <br>back-right, LFE-channel <br>8-15: 保留Reserved
original_copy|1| 	固定为0
home|1|	固定为0


adts_variable_header的字段|占用bit数|意义
-|-|-
copyright_identification_bit|1| 固定为0
copyright_identification_star|1| 固定为0
aac_frame_length|13| 一个ADTS帧的长度包括ADTS头和raw data block
adts_buffer_fullness|11| 固定为0x7ff ,说明是码率可变的码流
number_of_raw_data_blocks_in_frame|2| 	固定为00 。表示ADTS帧中有1个AAC数据块 <br>（注意：不是没有AAC数据块）


# h264
```text
1、从4.1.2开始支持java层的mediacodec
2、从5.0开始支持 native层的mediacodec
```

## h264常用的帧类型

类型| 说明
-|-
00 00 00 01 06 | 增强 帧
00 00 00 01 41 | 61和41其实都是P帧（type值为1），只是重要级别不一样
00 00 00 01 61 |  P帧
00 00 00 01 65 |  IDR 、I帧
00 00 00 01 67 |  SPS
00 00 00 01 68 | PPS


## H264查看工具
以二进制方式查看H264文件的工具可以用 [sublime_text](http://www.sublimetext.com/)

以图形化方式查看H264文件的I帧P帧等可以用[Elecard StreamEye ](http://opbrnhxqg.bkt.clouddn.com/Elecard-h264%E6%96%87%E4%BB%B6%E6%9F%A5%E7%9C%8B%E5%99%A8%E6%92%AD%E6%94%BE%E5%99%A8.rar)



## java层解码 显示 H264方案
```text
自己解析 H264协议，然后将数据作为一帧一帧地送给 java层的 mediacodec 解码 。
然后给 mediacodec 进行渲染显示 。
```

```text
/**
 @param surface Specify a surface on which to render the output of this
                decoder. Pass {@code null} as {@code surface} if the
                codec does not generate raw video output (e.g. not a video
                decoder) and/or if you want to configure the codec for
                {@link ByteBuffer} output.
*/
 mMeidaCodec.configure(mediaFormat, mSurfaceView.getHolder().getSurface(), null, 0);
```


## H264转MP4   Java实现方案
基于[mp4parser](https://github.com/sannies/mp4parser)  （ isoviewer-1.0-RC-35.jar）实现。

```text
/**
  此方案遇到Bug :
  Attempt to read from field 'boolean com.googlecode.mp4parser.
  h264.model.SeqParameterSet.residual_color_transform_flag' on
  a null object reference
  解决办法:在我的应用场景中，我是确保H264的头部保存了SPS 和 PPS .
  鉴于转换速度和此Bug 我后来放弃了这个方案，转用 C层使用ffmpeg的方案。
 * @param h264Path  h264文件绝对路径
 * @param mp4Path  mp4绝对路径
 * @return  是否转码成功
 */
@Deprecated
public static boolean  h264ToMp4Parse(String h264Path , String mp4Path ){
    try {
        H264TrackImpl h264Track = new H264TrackImpl(new FileDataSourceImpl(h264Path));
        Movie movie = new Movie();
        movie.addTrack(h264Track);
        Container mp4file = new DefaultMp4Builder().build(movie);
        FileChannel fc = new FileOutputStream(new File(mp4Path)).getChannel();
        mp4file.writeContainer(fc);
        fc.close();
        return  true ;
    }catch (Exception exception){
        //转码异常
        return  false ;
    }
}//
```
 
#  RTMP
RTMP（Real Time Messaging Protocol） 是Adobe的私有协议,未完全公开。基于TCP协议 。

```text
为什么网络直播一般采用rtmp？ 而安防等采用RTSP？
以下是我瞎猜的。
因为Adobe的flash一统天下的时候，RTMP是web平台的直播唯一方案。
各大CDN厂商付出了较多的努力实现了对RTMP比较稳定的支持。
后续的公司为了稳定会继续沿用。  
同理安防方面很多程序都是基于RTSP，因为生态也继续沿用。
```


# RTP
## RTP封包情况
### 单个NAL包单元
对于 NALU 的长度小于 MTU 大小的包, 一般采用单一 NAL 单元模式.

### 组合封包模式
当 NALU 的长度特别小时, 可以把几个 NALU 单元封在一个 RTP 包中. （这种情况用的少）

### FU-A的分片格式
而当 NALU 的长度超过 MTU 时, 就必须对 NALU 单元进行分片封包. 也称为 Fragmentation Units (FUs).
现存两个版本FU-A，FU-B。



## RTP 固定头结构
前 12 （第一排4个byte +第二排 4byte +第三排 4byte ）个字节出现在每个 RTP 包中。仅仅在被混合器插入时，才出现 CSRC 识别符列表。（如果CC为0，就代表没有CSRC）
![](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p1.png)


### RTP头定义说明
![p](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p2.png)


## NALU头 、FU indicator
```text
个人理解 NALU头和 FU indicator差不多 。
在单个封包模式下 RTP头后面跟着就是NALU头；在分片封包模式下，RTP头后面跟着的就是FU indicator。
NALU头和 FU indicator结构是一样的，只是Type(后五个bit)对应的意义不一样。
```

###  NALU头 、FU indicator 结构
由一个字节组成
![p](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p3.png)

![p](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p4.png)


## FU header
![p](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p5.png)
![p](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p6.png)



## 解析示例

```text
WkDrone项目用到的是 单个NAL包单元封装+ FUA分片封装模式。所以以此为例进行分析 。
1、客户端每次从服务器接收一个RTP包 。
RTP头 + 数据data

2、根据data[0] 判断是什么封包模式。

3、如果是单个NAL 就直接判断拿到整个NALU.
如果是FU_A模式 就根据data[1]判断是当前NALU单元是否开始、结束还是中间。
如果是开头就需要在开始接收前手动添加当前NALU类型（rtpPackage.data[0] & 0xE0)|(rtpPackage.data[1]&0x1F) -->
最后获得F 、NRI 、Type 这三个信息 ，F可以忽略， NRI标记这个NALU的重要性 ，Type标记帧类型）。

4、最后获得整个NAUL后 在前面00 00 00 01 传给解码器解码。
```



![p](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/wkDrone_p7.png)


```text
//关键代码：
/**
   * 将 rtp包 组成 NALU 格式的数据 <br>
   *     返回一帧H264数据(一个NALU)
   */
  public byte[] rtpPackage2NALU(byte[] rtpData)
  {
      if(rtpData.length == 0)
      {
          return null ;
      }
      RtpPackage rtpPackage = new RtpPackage();
      rtpPackage.V=(rtpData[0]&0xFF)>>6 ;
      rtpPackage.PT = rtpData[1] & 0x7F;
      rtpPackage.data = new byte[rtpData.length-12];
      System.arraycopy(rtpData, 12, rtpPackage.data, 0, rtpData.length - 12);
      if( (rtpPackage.V != 2)&&(rtpPackage.PT!=96) || rtpPackage.data.length <2 )
      {
          return null;
      }
      // rtp封包方式，单一封包、组合封包、分片封包等 0x1F -> 0001 1111
      int fuIndicatorTypeFlag = rtpPackage.data[0] & 0x1F;
      // FU head中的 S、E标记 用来判断NALU的开始、结束 0xCO ->  1100 0000  取S、E
      int fuHeadSEFlag = rtpPackage.data[1] & 0xC0;

      switch (fuIndicatorTypeFlag)
      {
          case NAL_UNIT_TYPE_STAP_A:
              break;
          case NAL_UNIT_TYPE_STAP_B:
              break;
          case NAL_UNIT_TYPE_MTAP16:
              break;
          case NAL_UNIT_TYPE_MTAP24:
              break;
          case NAL_UNIT_TYPE_FU_B:
              break;
          case NAL_UNIT_TYPE_FU_A:
              switch (fuHeadSEFlag)
              {
                  //NAL Unit start packet 0x80-> 1000 0000
                  case 0x80:
                      NALEndFlag = false;
                      bufferLength = rtpPackage.data.length-1 ;
                      buffer[0] = new byte[rtpPackage.data.length-1];
                      /*
                      每一帧开始的时候要加上表示这个帧类型的信息。
                      0xE0 ->1110 0000    0x1F -> 0001 1111
                      取FU indicator的前三位和FU Header的后五位。
                      最后这个byte包含了  F 、NRI 、Type 这三个信息
                      F可以忽略， NRI标记这个NALU的重要性 ，Type标记帧类型
                      buffer[0][0] 帧类型数据 41、61、65、67、68等
                       */
                      buffer[0][0] = (byte)((rtpPackage.data[0] & 0xE0)|(rtpPackage.data[1]&0x1F));
                      System.arraycopy(rtpPackage.data,2,buffer[0],1,rtpPackage.data.length-2);
                      packetNum = 1;
                      break;
                  //NAL Unit middle packet   0x00->0000 0000
                  case 0x00:
                      NALEndFlag = false;
                      bufferLength += rtpPackage.data.length-2;
                      buffer[packetNum] = new byte[rtpPackage.data.length-2];
                      System.arraycopy(rtpPackage.data,2,buffer[packetNum],0,rtpPackage.data.length-2);
                      packetNum++;
                      break;
                  //NAL Unit end packet  0x40 -->0100 0000
                  case 0x40:
                      NALEndFlag = true;
                      //+4 是因为多了 00 00 00 01
                      NALUnit = new byte[bufferLength + rtpPackage.data.length- 2 + 4];
                      //因为打包发送的时候是去掉了00 01这种标记位的，为了处理后续的H264，需要手动加上
                      NALUnit[0] = 0x00;
                      NALUnit[1] = 0x00;
                      NALUnit[2] = 0x00;
                      NALUnit[3] = 0x01;
                      int tmpLen = 4;
                      //第一片 + 中间片 数据
                      for(int i = 0; i < packetNum; ++i)
                      {
                          System.arraycopy(buffer[i],0,NALUnit,tmpLen,buffer[i].length);
                          tmpLen += buffer[i].length;
                      }
                      //最后一片的数据
                      System.arraycopy(rtpPackage.data,2,NALUnit,tmpLen,rtpPackage.data.length-2);
                      break;
              }
              break;

          //这里采用排除法，排除了组包封装和分片封装 之后就认为是 单个打包模式
          default:
              NALUnit = new byte[4+rtpPackage.data.length];
              NALUnit[0] = 0x00;
              NALUnit[1] = 0x00;
              NALUnit[2] = 0x00;
              NALUnit[3] = 0x01;
              System.arraycopy(rtpPackage.data,0,NALUnit,4,rtpPackage.data.length);
              NALEndFlag = true;
              break;
      }
      if(NALEndFlag)
      {
         return  NALUnit;
      }else{
          return  null;
      }
  }
```


## 个人经验
- 1、以java为例，加入你不知道当前数据要与0X1F进行与运算，你可以先想想用哪个二进制可以达到想要的目的，然后将这个二进制转为十六进制就可以了。
```text
// rtp封包方式，单一封包、组合封包、分片封包等 0x1F -> 0001 1111
int fuIndicatorTypeFlag = rtpPackage.data[0] & 0x1F;
```


# rtsp
```html
RTSP被用于建立的控制媒体流的传输 。
Rtp协议提供了时间戳和序列号。发送端在采样时设置时间戳，接收端收到后，会按照时间戳依次播放。

RTP本身只保证实时数据的传输，并不能为按顺序传送的数据包提供可靠的传送机制，也不提供流量和拥塞控制，
它依靠RTCP来提供这些服务。

RTCP通常与RTP配合使用，用以管理传输质量在当前进程之间的交换信息。
在RTP会话期间，各参与者周期性的传送RTCP包，RTCP包中包含已发送数据包的数量、丢失的数据包的数量等统计资料。
服务器可以利用这些信息动态的改变传输速率，甚至改变有效载荷的类型。
RTSP通常使用RTP协议来传送实时流，RTP一般使用偶数端口，而RTCP使用相邻的奇数端口，即RTP端口号+1。

注意: RTP 本身没有提供任何的机制来确保实时的传输或其他的服务质量保证，而是由底层的服务来完成。
它不保证传输或防止乱序传输,它不假定下层网络是否可靠,是否按顺序传送数据包。
```



## RTP、RTSP、RTCP的区别
```text
Rtsp
进行服务端和客户端的信息交流。
例如客户端询问服务器支持哪些方法、协议、sps、pps等。沟通以什么方式建立连接，是否要开始发送数据了等。

Rtp
服务器以约定好的格式往客户端进行发送封装好的数据。

RTCP
当客户端发现RTP丢包的时候，可以通过约定好的格式往服务器发送数据，要求服务器重发数据。
注：虽然UDP是单向通信的， 但是 RTP + RTCP 其实相当于建立起了一个双向对话的机制了。
```


##   rtsp 的交互过程 （客户端和服务端）
```text 
1、OPTIONS（可选）
RTSP客户端   —> RTSP服务器端  OPTIONS命令      //询问服务器端有哪些方法可使用
RTSP服务器端 —> RTSP客户端  回复OPTIONS命令    //回复客户端服务器支持的方法  

2、DESCRIBE （可选）
RTSP客户端   —>  RTSP服务器端   DESCRIBE命令      //请求对媒体资源的描述信息
RTSP服务器端 —>  RTSP客户端 　　回复DESCRIBE命令　//回复客户端某个媒体资源的描述信息（即SDP）

3、SETUP（必选）
RTSP客户端   —> RTSP服务器端　SETUP命令       //请求建立对某个媒体资源的连接
RTSP服务器端 —>  RTSP客户端　 回复SETUP命令　 //回复建立连接的结果
　　
4、PLAY（必选）
RTSP客户端    —>  RTSP服务器端　 PLAY命令　　　   //请求播放媒体资源
RTSP服务器端  —>  RTSP客户端　　 回复PLAY命令　　 //回复播放的结果

5、其他的交互方法，请自行查阅 。
例如 TEARDOWN、PAUSE、SCALE、 GET_PARAMETER、SET_PARAMETER 　　
```

## 搭建、测试rtsp交互
可以采用以下方案搭建一套系统。

### 采集、推流
用[EasyPusher](https://github.com/EasyDSS/EasyPusher-Android) , 商业需要授权。

### 流媒体服务器
用[EasyDarwin ](https://github.com/EasyDarwin/EasyDarwin/tree/master/EasyDarwin) ， 完全免费。

### 解码播放
用 VLC ，完全免费。


##  解rtsp协议获得H264 ，java实现demo
```text 
public class RtspClient{
    private final String TAG="RtspClient" ;
    /**是否是tcp方式*/
    private boolean isTCPtranslate;
    private String hostIp;
    private int port;
    private String rtspUrl ;
    /**rtsp版本号，必须是1.0? 还不确定！*/
    private static final String VERSION = " RTSP1.0\r\n";
    /**rtsp服务器正确反应标记*/
    private static final String RTSP_OK = "RTSP/1.0 200 OK";
    /**请求回应对的序列*/
    private int seq=1;
    /**会话 Session在setup请求后返回  */
    private String sessionid=null;
    /**trackID 在 请求Descrbe 之后返回*/
    private String trackInfo;
    /**转换账号和密码*/
    private String authorBase64;
    /**客户端代理名称随意填*/
    private final static String UserAgent = "RtspWalkera/1.0";
    private Socket mSocket;
    private BufferedReader mBufferreader;
    private OutputStream mOutputStream;
    /**
     * 构造函数 ， 进行参数初始化
     * @param isTCPtranslate tcp方式还是 udp方式
     * @param hostIp  服务器ip
     * @param port  服务器端口
     * @param rtspUrl  rtsp播放地址
     * @param authorName 登录账号
     * @param authorPassword 登录密码
     */
    public RtspClient(boolean isTCPtranslate, String hostIp,int port,
    String rtspUrl ,String authorName,String authorPassword)  {
        this.isTCPtranslate = isTCPtranslate;
        this.rtspUrl = rtspUrl;
        this.hostIp = hostIp ;
        this.port = port;
        if(authorName == null && authorPassword == null) {
            authorBase64 = null;
        }  else {
            // 返回Base64编码过的字节数组字符串
            authorBase64 = Base64.encodeToString((authorName+":"+authorPassword).getBytes(),Base64.DEFAULT);
        }
    }

    private String addHeaders(){
        return "CSeq: " + (seq++) + "\r\n"
                + ((authorBase64 == null)?"":("Authorization: Basic " +authorBase64 +"\r\n"))
                + "User-Agent: " + UserAgent + "\r\n"
                + ((sessionid == null)?"":("Session: " + sessionid + "\r\n"))
                + "\r\n";
    }

    /**
     * option请求 <br>
     *     此步骤非必须
     */
    public void doOption(){
        StringBuilder sb = new StringBuilder();
        sb.append("OPTIONS ");
        sb.append(this.rtspUrl.substring(0, rtspUrl.lastIndexOf("/")));
        sb.append(VERSION);
        sb.append(addHeaders());
        try{
            outQueue.put(sb.toString());
        }catch (Exception e)  {}
    }

    /**
     *  Describe 请求<br>
     *  sps 和 pps 会在这个阶段返回
     *  sprop-parameter-sets= Z0IAKpY1QMgEvTcBAQEC,aM48gA==
     *  将  sprop-parameter-sets 对应的值用base64 解码即可 67 和68
     *  分别对应sps 和pps . <br>
     *      不过有的rtsp服务器如果没有按标准写的话 ，sprop-parameter-sets 返回的值可能为空。
     *      此时 如果想获取sps 和pps 只能通过rtsp服务器返回的 帧数据中去获取了。
     */
    public void doDescribe()  {
        StringBuilder sb = new StringBuilder();
        sb.append("DESCRIBE ");
        sb.append(this.rtspUrl);
        sb.append(VERSION);
        sb.append(addHeaders());
        try  {
            outQueue.put(sb.toString());
        }catch (Exception e)  {}
    }

    /**
     * 请求setup 。 <br>
     * RTP/AVP/TCP : RTP采用TCP方式 <br>
     * unicast表示单播<br>
     * client_port=55640-55641 ：前一个是RTP端口 ，后一个是RTCP端口<br>
     *
     */
    public void doSetup( )  {
        StringBuilder sb = new StringBuilder();
        if ( isTCPtranslate ) {
            sb.append("SETUP ");
            sb.append(this.rtspUrl);
            sb.append("/");
            sb.append(trackInfo);
            sb.append(VERSION);
            sb.append("Transport: RTP/AVP/TCP;unicast;client_port=55640-55641" + "\r\n");
            sb.append(addHeaders());
        } else {
            sb.append("SETUP ");
            sb.append(this.rtspUrl);
            sb.append("/");
            sb.append(trackInfo);
            sb.append(VERSION);
            sb.append("Transport: RTP/AVP/UDP;unicast;client_port=55640-55641" + "\r\n");
            sb.append(addHeaders());
        }
        try  {
            outQueue.put(sb.toString());
        }catch (Exception e)  {  }
    }

    /**
     * play 请求
     */
    public void doPlay(){
        StringBuilder sb = new StringBuilder();
        sb.append("PLAY ");
        sb.append(this.rtspUrl);
        sb.append(VERSION);
        //设置播放的时间范围
        sb.append("Range: npt=0.000-\r\n");
        sb.append(addHeaders()) ;
        try  {
            outQueue.put(sb.toString());
        }catch (Exception e)  {  }
    }

    /**
     * Pause 暂停请求
     */
    public void doPause() {
        StringBuilder sb = new StringBuilder();
        sb.append("PAUSE ");
        sb.append(this.rtspUrl);
        sb.append("/");
        sb.append(VERSION);
        sb.append(addHeaders()) ;
        try{
            outQueue.put(sb.toString());
        }catch (Exception e)  {  }
    }

    /**
     * 关闭连接
     */
    public void doTeardown(){
        StringBuilder sb = new StringBuilder();
        sb.append("TEARDOWN ");
        sb.append(this.rtspUrl);
        sb.append("/");
        sb.append(VERSION);
        sb.append(addHeaders()) ;
        try  {
            outQueue.put(sb.toString());
        }catch (Exception e){ }
    }

    /**
     * 往服务器发送数据
     */
    private void sendBytes(String questStr)  {
        try  {
            byte[] buffer= questStr.getBytes("UTF-8");
            mOutputStream.write(buffer);
            mOutputStream.flush();
            Log.i(TAG ,"请求服务器："+questStr +" "+outQueue.size());
        }catch (Exception e)
        {
            Log.i(TAG ,"请求服务器异常："+e.getMessage());
        }
    }

    private boolean isStopReceive =false ;
    /***
     * rtsp 接收子线程
     */
    private class ReceiveThread extends Thread{
        @Override
        public void run()  {
            while(!isStopReceive)  {
                try  {
                    if(mBufferreader!=null)
                    {
                        String line  ;
                        if( (line = mBufferreader.readLine()) != null)
                        {
                            Log.i(TAG ,"收到服务器返回=" + line);
                        }else {
                            Log.i(TAG ,"收到服务器返回 line==null" );
                        }
                    }
                    //休眠一段时间
                    Thread.sleep(60);
                }catch (Exception e)
                {
                    Log.i(TAG ,"接收服务器异常" +e.getLocalizedMessage());
                }
            }
        }
    }

    private LinkedBlockingDeque<byte[]> rtpDataBuffer = new LinkedBlockingDeque<>();
    private DatagramSocket mUdpSocket;
    private DatagramPacket mUdpPackets;
    int udpPort = 55640; //55640- 55641
    private byte[] message = new byte[2048];
    /***
     * upd rtp 接收子线程
     */
    private class ReceiveRTPUdpThread extends Thread  {
        @Override
        public void run(){
            while(!isStopReceive)  {
                try  {
                    if(mUdpSocket==null)
                    {
                        mUdpSocket = new DatagramSocket(udpPort);
                        mUdpPackets = new DatagramPacket(message,message.length);
                    }else{
                        mUdpSocket.receive(mUdpPackets);
                        byte[] buffer = new byte[mUdpPackets.getLength()];
                        System.arraycopy(mUdpPackets.getData(), 0, buffer, 0, mUdpPackets.getLength());
                        try {
                            rtpDataBuffer.put(buffer);
                        } catch (InterruptedException e) {
                            Log.e(TAG,"The buffer queue is full , wait for the place..");
                        }
                    }

                    //休眠一段时间
                    Thread.sleep(0);
                }catch (Exception e)
                {
                    Log.i(TAG ,"udp接收服务器异常" +e.getLocalizedMessage());
                }
            }
        }
    }

    public byte[]getRtpData(){
        byte[] nalu =null;
        if(rtpDataBuffer !=null && rtpDataBuffer.size()>0){
            try {
                nalu = rtpDataBuffer.take();
            }catch (Exception e)  {  }

        }
        return  nalu ;
    }

    /**发送队列*/
    private BlockingQueue<String> outQueue = new LinkedBlockingQueue< >(Integer.MAX_VALUE);;
    /**
     * 发送子线程
     */
    private class SendThread extends  Thread  {
        @Override
        public void run()  {
            while(!isStopReceive)  {
                try  {
                    String buffer  = outQueue.poll();
                    if (buffer != null){
                        sendBytes(buffer);
                    }
                    //休眠一段时间
                    Thread.sleep(60);
                }catch (Exception e)  {}
            }
        }
    }

    /**
     * 开启rtsp客户端
     */
    public void openRtspCline()  {
        isStopReceive = false ;
        Thread  thread = new Thread(new Runnable() {
            @Override
            public void run() {
                try{
                    mSocket = new Socket(hostIp, port);
                    mBufferreader = new BufferedReader(new InputStreamReader(mSocket.getInputStream()));
                    mOutputStream = mSocket.getOutputStream();
                }catch (IOException e)
                {
                    Log.i(TAG,"创建socket出现异常="+e.getLocalizedMessage()) ;
                }
            }
        });
        thread.start();

        ReceiveThread receiveThread = new ReceiveThread();
        receiveThread.start();
        SendThread sendThread = new SendThread() ;
        sendThread.start();

        ReceiveRTPUdpThread receiveRTPUdpThread = new ReceiveRTPUdpThread();
        receiveRTPUdpThread.start();
    }

    /**
     * 关闭rtsp客户端
     */
    public void shutDownRtspClient()  {
        isStopReceive = true ;
    }
}
```

```text
/**
 *  RTP 数据包 结构
 */
public class RtpPackage{
    /**版本是 2*/
    public int V ;
    /**填充标志*/
    public int P ;
    /**拓展标志*/
    public int X ;
    /**csrc计数器*/
    public int CC ;
    /**音频、视频标记*/
    public int M ;
    /**多媒体的类型 H.264的类型为96*/
    public int PT;
    /**序列号*/
    public int sequenceNumber;
    /**时间戳*/
    public long timestamp;
    /**ssrc*/
    public long ssrcIdentifier;
    /**csrc列表*/
    public int csrcIdentiferList ;
    /**rtp 包数据*/
    public byte[] data;
}
```


```text
public   class RTP2NaluTools{
    private final String TAG="RTP2NaluTools" ;
    private final static int NAL_UNIT_TYPE_STAP_A = 24;
    private final static int NAL_UNIT_TYPE_STAP_B = 25;
    private final static int NAL_UNIT_TYPE_MTAP16 = 26;
    private final static int NAL_UNIT_TYPE_MTAP24 = 27;
    private final static int NAL_UNIT_TYPE_FU_A = 28;
    private final static int NAL_UNIT_TYPE_FU_B = 29;
    /**一帧数据（一个NALU）*/
    private byte[] NALUnit;
    /**NALU结束标记*/
    private boolean NALEndFlag;
    /**临时变量 buffer[i] 存储每一片数据 */
    private byte[][] buffer = new byte[1024][];
    /**当前已经接收的NALU的数据长度*/
    private int bufferLength;
    /**当前已经接收的数据片数量*/
    private int packetNum;
    public RTP2NaluTools()  {
        NALEndFlag = false;
        bufferLength= 0;
        packetNum =0;
    }

    /**
     * 将 rtp包 组成 NALU 格式的数据 <br>
     *     返回一帧H264数据(一个NALU)
     */
    public byte[] rtpPackage2NALU(byte[] rtpData)  {
        if(rtpData.length == 0)  {
            return null ;
        }
        RtpPackage rtpPackage = new RtpPackage();
        rtpPackage.V=(rtpData[0]&0xFF)>>6 ;
        rtpPackage.PT = rtpData[1] & 0x7F;
        rtpPackage.data = new byte[rtpData.length-12];
        System.arraycopy(rtpData, 12, rtpPackage.data, 0, rtpData.length - 12);
        if( (rtpPackage.V != 2)&&(rtpPackage.PT!=96) || rtpPackage.data.length <2 )  {
            return null;
        }
        // rtp封包方式，单一封包、组合封包、分片封包等 0x1F -> 0001 1111
        int fuIndicatorTypeFlag = rtpPackage.data[0] & 0x1F;
        // FU head中的 S、E标记 用来判断NALU的开始、结束 0xCO ->  1100 0000  取S、E
        int fuHeadSEFlag = rtpPackage.data[1] & 0xC0;

        switch (fuIndicatorTypeFlag)  {
            case NAL_UNIT_TYPE_STAP_A:
                break;
            case NAL_UNIT_TYPE_STAP_B:
                break;
            case NAL_UNIT_TYPE_MTAP16:
                break;
            case NAL_UNIT_TYPE_MTAP24:
                break;
            case NAL_UNIT_TYPE_FU_B:
                break;
            case NAL_UNIT_TYPE_FU_A:
                switch (fuHeadSEFlag)  {
                    //NAL Unit start packet 0x80-> 1000 0000
                    case 0x80:
                        NALEndFlag = false;
                        bufferLength = rtpPackage.data.length-1 ;
                        buffer[0] = new byte[rtpPackage.data.length-1];
                        /*
                        每一帧开始的时候要加上表示这个帧类型的信息。
                        0xE0 ->1110 0000    0x1F -> 0001 1111
                        取FU indicator的前三位和FU Header的后五位。
                        最后这个byte包含了  F 、NRI 、Type 这三个信息
                        F可以忽略， NRI标记这个NALU的重要性 ，Type标记帧类型
                        buffer[0][0] 帧类型数据 41、61、65、67、68等
                         */
                        buffer[0][0] = (byte)((rtpPackage.data[0] & 0xE0)|(rtpPackage.data[1]&0x1F));
                        System.arraycopy(rtpPackage.data,2,buffer[0],1,rtpPackage.data.length-2);
                        packetNum = 1;
                        break;
                    //NAL Unit middle packet   0x00->0000 0000
                    case 0x00:
                        NALEndFlag = false;
                        bufferLength += rtpPackage.data.length-2;
                        buffer[packetNum] = new byte[rtpPackage.data.length-2];
                        System.arraycopy(rtpPackage.data,2,buffer[packetNum],0,rtpPackage.data.length-2);
                        packetNum++;
                        break;
                    //NAL Unit end packet  0x40 -->0100 0000
                    case 0x40:
                        NALEndFlag = true;
                        //+4 是因为多了 00 00 00 01
                        NALUnit = new byte[bufferLength + rtpPackage.data.length- 2 + 4];
                        //因为打包发送的时候是去掉了00 01这种标记位的，为了处理后续的H264，需要手动加上
                        NALUnit[0] = 0x00;
                        NALUnit[1] = 0x00;
                        NALUnit[2] = 0x00;
                        NALUnit[3] = 0x01;
                        int tmpLen = 4;
                        //第一片 + 中间片 数据
                        for(int i = 0; i < packetNum; ++i)
                        {
                            System.arraycopy(buffer[i],0,NALUnit,tmpLen,buffer[i].length);
                            tmpLen += buffer[i].length;
                        }
                        //最后一片的数据
                        System.arraycopy(rtpPackage.data,2,NALUnit,tmpLen,rtpPackage.data.length-2);
                        break;
                }
                break;

            //这里采用排除法，排除了组包封装和分片封装 之后就认为是 单个打包模式
            default:
                NALUnit = new byte[4+rtpPackage.data.length];
                NALUnit[0] = 0x00;
                NALUnit[1] = 0x00;
                NALUnit[2] = 0x00;
                NALUnit[3] = 0x01;
                System.arraycopy(rtpPackage.data,0,NALUnit,4,rtpPackage.data.length);
                NALEndFlag = true;
                break;
        }
        if(NALEndFlag)  {
           return  NALUnit;
        }else{
            return  null;
        }
    }
    public void stop()  {
        NALUnit = null;
    }
}

```

### 使用方法
```text
boolean isTCPtranslate = false ; //false  true
String hostIp ="192.168.1.235";
int port= 554 ;
String rtspUrl ="rtsp://192.168.1.235:554/";
String authorName=null ;
String authorPassword=null ;
RtspClient rtspClient = new RtspClient( isTCPtranslate,  hostIp, port,  rtspUrl , authorName, authorPassword);
rtspClient.openRtspCline();

//请求options
rtspClient.doOption();
//请求describe
rtspClient.doDescribe();
//请求建立连接(此阶段指明用Tcp或UDP建立连接)
rtspClient.doSetup();
//请求开始下发Rtp包
rtspClient.doPlay();

//开启子线程接收RTP数据
 byte[] rtpData =rtspClient.getRtpData() ;
//对Rtp数据进行解析，获得H264数据
byte[] nalu= rtp2NaluTools.rtpPackage2NALU(rtpData) ;
```


#  YUV
与RGB类似，YUV是一种颜色编码方法，它将亮度信息（Y）与色彩信息（UV）分离，没有UV信息一样可以显示完整的（黑白的）图像。  YUV图像查看工具 [yuvplayer.exe]()
```text
Y ：    明亮度（Luminance或Luma），也就是灰度值；
U和V ： 色度（Chrominance或Chroma），描述影像色彩及饱和度，用于指定像素的颜色。
```

## YUV数据的采集方式

采集方式 | 特点 | 其他
- | - |-
YUV 4:4:4|每一个Y对应一组UV分量 | YUV444
YUV 4:2:2|每两个Y共用一组UV分量 | YUV422
YUV 4:2:0|每四个Y共用一组UV分量 | YUV420


## yuv格式类型

YUV 格式 | 排列 | 所属大类
- | -
YU12|YYYY YYYY UU VV|YUV420
I420|YYYY YYYY UU VV|YUV420
YV12 |YYYY YYYY VV UU|YUV420
NV12|YYYY YYYY UV UV|YUV420
NV21|YYYY YYYY VU VU|YUV420
YUYV |YUYV YUYV YUYV YUYV|YUV422
YUY2|YUYV YUYV YUYV YUYV|YUV422
UYVY|UYVY UYVY UYVY UYVY|YUV422
YUV422P|YYYY YYYY UUUU VVVV|YUV422


# 音视频 开发要点

## 音频开发的主要内容
```text
1、音频采集/播放
2、音频算法处理
   去噪、静音检测、回声消除、音效处理、功放/增强、混音/分离
3、音频的编解码和格式转换
4、音频传输协议的开发
SIP，A2DP、AVRCP
```

## 音频开发的延时标准
```c++
ITU-TG.114规定，对于高质量语音可接受的时延是300ms。
一般来说，如果时延在300～400ms，通话的交互性比较差，但还可以接受。
时延大于400ms时，则交互通信非常困难。
```

## 音频开发的难点
```text
延时敏感、卡顿敏感、噪声抑制（Denoise）、
回声消除（AEC）、静音检测（VAD）、混音算法
```

### 常用显示控件
```text
// SurfaceView  
Android 1.0 ,  extends View 。本质上是一个View。与普通View不同的是，它有自己的Surface。

// GLSurfaceView  
Android 1.5  ， extends SurfaceView 。 在SurfaceView的基础上，它加入了EGL的管理，
并自带了渲染线程。另外它定义了用户需要实现的Render接口。

// SurfaceTexture  
Android 3.0 ， extends 无 。 它对图像流的处理并不直接显示，而是转为GL外部纹理，
因此可用于图像流数据的二次处理（如Camera滤镜，桌面特效等）

// TextureView  
Android 4.0 ，extends View 。 普通View，可以和其它普通View一样进行移动，旋转，缩放，动画等变化。
TextureView必须在硬件加速的窗口中。
android:hardwareAccelerated="true"

// Surface  
Android 1.0 ， extends 无  。 
Surface 内部维护了图像 buffer 对象，最终交由 SufaceFlinger 合成显示。
```



## 音视频术语
### 采样率（samplerate）
```text
采样频率越高，记录这一段音频信号所用的数据量就越大，同时音频质量也就越高。
根据奈奎斯特理论，采样频率只要不低于音频信号最高频率的两倍，就可以无损失地还原原始的声音。

为了保证声音不失真，采样频率应在40kHz以上。
常用的音频采样频率有：
8kHz、11.025kHz、22.05kHz、16kHz、37.8kHz、44.1kHz、48kHz、96kHz、192kHz
```


### 量化精度（位宽）
```text
位数越多，表示得就越精细，声音质量就越好，数据量也会越大。

数据类型大小可以是：
4bit、8bit、16bit、32bit
常见的位宽是：8bit 或者 16bit
```

### 复用/解复用： 
```text
将音频、视屏、字幕数据放在一条链路上传过来，然后再想办法分开。
```

### 声道数（channels）
```text
由于，音频的采集和播放是可以叠加的，
因此可以同时从多个音频源采集声音，并分别输出到不同的扬声器，
故声道数一般表示声音录制时的音源数量或回放时相应的扬声器数量。

单声道（Mono）和双声道（Stereo）比较常见。
```

###  重采样
```text
把目标音频按照一定的格式重新采样编码成新的音频数据，
方便统一处理，一般的采样标准是：44100HZ、16bit、双声道 。

因为需要播放的音频并不是一样的，
我们也不可能为每一种音频写一个处理程序，所以要按照自己的标准重新采样。
```

###  声道布局
```text
当有多个发音源时，多个扬声器的摆放位置可以组成那种前后、左右、环绕等布局...
```


###  音频帧
```text
音频数据是流式的，本身没有明确的一帧帧的概念，
在实际的应用中，为了音频算法处理/传输的方便，一般约定俗成取2.5ms~60ms为单位的数据量为一帧音频。

这个时间被称之为“采样时间”，其长度没有特别的标准，它是根据编解码器和具体应用的需求来决定的，
我们可以计算一下一帧音频帧的大小：

假设某音频信号是采样率为8kHz、双通道、位宽为16bit，20ms一帧，则一帧音频数据的大小为：
int size = 8000 x 2 x 16bit x 0.02s = 5120 bit = 640 byte
```

###  音调 、音速
```text
音调(Pitch)：音调与声音的频率有关 ， 声音频率越大时，音调就越高 。
音速 ：播放速度。
```


### 解协议
```text
就是将流媒体协议的数据，解析为标准的相应的封装格式数据。
视音频在网络上传播的时候，常常采用各种流媒体协议，例如HTTP，RTMP，或是MMS等等。
这些协议在传输视音频数据的同时，也会传输一些信令数据。
这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等。
解协议的过程中会去除掉信令数据而只保留视音频数据。例如，采用RTMP协议传输的数据，经过解协议操作后，输出FLV格式的数据。
```

### 解封装
```text
就是将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据。
封装格式种类很多，例如MP4，MKV，RMVB，TS，FLV，AVI等等，它的作用就是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。
例如，FLV格式的数据，经过解封装操作后，输出H.264编码的视频码流和AAC编码的音频码流。
```


### 解码
```text
解码是整个系统中最重要也是最复杂的一个环节。
就是将视频/音频压缩编码数据，解码成为非压缩的视频/音频原始数据。
音频的压缩编码标准包含AAC，MP3，AC-3等等，视频的压缩编码标准则包含H.264，MPEG2，VC-1等等。
通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，
例如YUV420P，RGB等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如PCM数据。
```

### 视音频同步
```text
就是根据解封装模块处理过程中获取到的参数信息，
同步解码出来的视频和音频数据，并将视频音频数据送至系统的显卡和声卡播放出来。
```


### 网络抖动
抖动是指最大延迟与最小延迟的时间差，如最大延迟是20毫秒，最小延迟为5毫秒，那么网络抖动就是15毫秒，它主要标识一个网络的稳定性。

###  MTU
```text
Maximum Transmission Unit 。大部分设备的是1500byte。
如果本机的MTU比网关的MTU大，大的数据包就会被拆开来传送，这样会产生很多数据包碎片，增加丢包率，降低网络速度。
把本机的MTU设成比网关的MTU小或相同，就可以减少丢包。
```

### I帧
```text
关键帧。 数据量较大。
这一帧单独存在，不需要依赖其他的帧就可以完整显示。
```

### P帧 、差别帧
```text
记录这一帧跟之前的一个帧 （I帧或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。
P帧没有完整画面数据，只有与前一帧的画面差别的数据。
```

### B帧 、双向差别帧
```text
B帧记录的是本帧与前后帧的差别。
要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。
B帧压缩率高，但是解码耗资源、算法相对复杂。
```


## 音视频格式
### 音频格式相关

格式| 说明 | 类型
-|-|-
pcm|录音后直接得到的未经压缩的数据流，没有头信息或者帧信息 <br> 可以使用 AUDACITY这个软件 播放pcm 文件| 编码格式
WAV|由文件头部分和音频数据体组成。微软开发的一种无损的音频文件格式 | 封装格式
MP3|每个帧之间相互独立，各帧相互独立。是有损压缩| 封装格式
m4a| | 封装格式
AAC|是有损压缩|封装格式
AMR||封装格式
CAF||苹果的一种音频封装格式


### 当前主要的流媒体协议

名称|推出机构|传输层协议|客户端|目前使用领域
-|-|-|-|-
RTSP + RTP| IETF|TCP+UDP|VLC ,WMP|IPTV
RTMP|Adobe Inc| TCP|flash|互联网直播
RTMFP|Adobe Inc|UDP|flash|互联网直播
MMS| microsoft Inc|TCP/UDP|WMP|互联网直播 +点播
HTTP|WWW+IETF|TCP|flash|互联网点播

### 视频封装格式
名称|推出机构|流媒体|支持的视频编码|支持的音频编码|目前使用领域
-|-|-|-|-|-
AVI|microsoft Inc|不支持|几乎所有格式|几乎所有格式|BT下载影视
MP4|MPEG|支持|MPEG-2 , MPEG-4 <br> H.264,H.263|AAC ,MPEG-1 Layers Ⅰ, Ⅱ,Ⅲ <br> AC-3|互联网视频网站
TS|MPEG|支持|MPEG-1 ,MPEG-2 ,<br>MPEG-4 ,H.264|MPEG-1 Layers Ⅰ, Ⅱ,Ⅲ <br> AAC   |IPTV，数字电视
FLV|adobe inc|支持|sorenson,VP6 ,H.264|MP3,ADPCM ,<br>Linear PCM ,AAC|互联网视频网站
MKV|corecodec inc|支持|几乎所有格式|几乎所有格式|互联网视频网站
RMVB|real networks inc|支持|RealVideo 8,9,10|AAC ,Cook Codec ,<br>RealAudio Lossless|BT下载影视


### 视频编码格式
名称|推出机构|推出时间|目前使用领域
-|-|-|-
H.264|MPEG/ITU-T|2003|各个领域
HEVC(H.265)|MPEG/ITU-T|2013|研发中
MPEG2|MPEG|1994|数字电视
MPEG4|MPEG|2001|不温不火
VP8|Google|2008|不普及
VP9|Google|2013|研发中
VC-1|Microsoft Inc.|2006|微软平台
wmv|||

## wav文件格式
在文件的前44字节放置标头，使播放器或编辑器能够简单掌握文件的基本信息，其内容以区块(chunk)为最小单位，每一区块长度为4字节。

名称 | 字节| 端序 | 内容
-|-|-|-
ChunkID 区块编号 |4 | 大端 | "RIFF"
ChunkSize 总区块大小|4 |小端 |
Format 文件格式| 4| 大端 | WAVE
Subchunk1Id 子区块1编号| 4| 大端 | "fmt "
Subchunk1Size 子区块1大小| 4| 小端 |16
AudioFormat 音频格式| 2| 小端 | 1 pcm
NumChannels 声道数量| 2| 小端 | 1 单声道 ，2 立体声
SampleRate 采样率| 4| 小端 |常用的采样频率有 11025, 22050 和 44100 kHz
ByteRate 每秒数据字节数。码率？| 4| 小端 |
BlockAlign 区块对齐| 2| 小端 | 声道数 * 每个采样需要的bit  / 8
BitsPerSample 采样位数| 2| 小端 | 
Subchunk2ID 子区块2编号|4 | 大端 |
Subchunk2Size 子区块2大小 |4 | 小端 |
data 音频数据|n|  小端 |
