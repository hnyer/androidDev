# QT
QT是一套UI解决方案。一般用来开发桌面程序或者嵌入式。
读者经常将MFC和Qt进行对比，MFC只能应用 Windows平台，而Qt是跨平台的。
另外，Qt已经封装了底层细节，学习Qt将会非常简单；而MFC只是 Windows API加了一层包装，不了解WindowsAPI也学不好 MFC。

# SDL
```text
SDL通过不同平台（例如Windows、linux、android）的屏幕驱动，封装成一套对外统一的API调用。
让使用者可以不关注具体某个平台，可以快速开发图像的绘制操作。SDL的核心，便是如此。
SDL的主要任务，便是完成图像的加载，渲染显示，其他方向都比较单薄。
```

# OpenCV
OpenCV 采用C及C++语言编写，可以在windows, linux, mac OSX系统上面运行。完全是免费的。
[opencv官网](http://opencv.org/downloads.html)
[opencv中文网站](http://www.opencv.org.cn/portal.php )

# ffmpeg
[ffmpeg官网](http://ffmpeg.org/) <br>
[ffmpeg Github](https://github.com/FFmpeg/FFmpeg) <br>
[官方英文文档](http://ffmpeg.org/documentation.html) <br>
[ffmpeg参数 文档](http://ffmpeg.org/ffmpeg.htm)

##  FFmpeg主要模块
```html
libavcodec：    用于各种类型声音/图像编解码；
libavdevice：   用于视频采集访问摄像头等
libavfilter：   用做滤镜处理
libavformat：   用于各种音视频封装格式的生成和解析，包括获取解码所需信息以生成解码
                上下文结构和读取音视频帧等功能；
libswresample： 用于重采样；
libavutil：     包含一些公共的工具函数；
libswscale：    用于视频场景比例缩放、色彩映射转换；
libpostproc：   用于后期效果处理；
```


## 移植ffmpeg到安卓
[参考资料](https://blog.csdn.net/ywl5320/article/details/75136986)
### 编译ffmpeg得到so等相关文件
```text
1、在ubuntu下配置好ndk环境
2、下载合适的版本下载(ffmpeg-3.3.7.tar.gz) http://ffmpeg.org/download.html#releases
3、解压，赋予最高权限

4、由于ffmpeg默认生成的格式是 xxx.so.版本号, android只能识别 xxx.版本号.so
所以需要修改 configure 这个文件
# 修改前
#SLIBNAME_WITH_MAJOR='$(SLIBNAME).$(LIBMAJOR)'
#LIB_INSTALL_EXTRA_CMD='$$(RANLIB) "$(LIBDIR)/$(LIBNAME)"'
#SLIB_INSTALL_NAME='$(SLIBNAME_WITH_VERSION)'
#SLIB_INSTALL_LINKS='$(SLIBNAME_WITH_MAJOR) $(SLIBNAME)'
# 修改后
SLIBNAME_WITH_MAJOR='$(SLIBPREF)$(FULLNAME)-$(LIBMAJOR)$(SLIBSUF)'
LIB_INSTALL_EXTRA_CMD='$$(RANLIB) "$(LIBDIR)/$(LIBNAME)"'
SLIB_INSTALL_NAME='$(SLIBNAME_WITH_MAJOR)'
SLIB_INSTALL_LINKS='$(SLIBNAME)'

5、在 相同目录下新建一个文件build_android.sh
#!/bin/bash
make clean
export NDK=/home/aivin/aivinSpace/ndk-r10e/android-ndk-r10e
export SYSROOT=$NDK/platforms/android-9/arch-arm/
export TOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.8/prebuilt/linux-x86_64
export CPU=arm
export PREFIX=$(pwd)/android/$CPU
export ADDI_CFLAGS="-marm"

./configure --target-os=linux \
--prefix=$PREFIX --arch=arm \
--enable-shared \
--disable-static \
--disable-yasm \
--disable-symver \
--enable-gpl \
--enable-ffmpeg \
--disable-ffplay \
--disable-ffprobe \
--disable-ffserver \
--disable-doc \
--disable-symver \
--cross-prefix=$TOOLCHAIN/bin/arm-linux-androideabi- \
--enable-cross-compile \
--sysroot=$SYSROOT \
--extra-cflags="-Os -fpic $ADDI_CFLAGS" \
--extra-ldflags="$ADDI_LDFLAGS" \
$ADDITIONAL_CONFIGURE_FLAG
make clean
make
make install

6、在build_android.sh 目录下用终端输入  ./build_android.sh
执行完毕后在当前目录下生成 android文件夹。
```

### 以下是网友给的一个脚本
```text
先修改 configure文件
然后直接执行 build_android.sh文件（不用执行configure文件）
//在3.3.7 、3.3.8版本中可以编译通过，在3.4.4通不过
--target-os=linux
// 所以我全部改用 android配置
--target-os=android
```

```text
#!/bin/bash
export NDK_HOME=/home/aivin/aivinSpace/ndk-r10e/android-ndk-r10e
export PLATFORM_VERSION=android-9
function build
{
	echo "start build ffmpeg for $ARCH"
	./configure --target-os=android \
	--prefix=$PREFIX --arch=$ARCH \
	--disable-doc \
	--enable-shared \
	--disable-static \
	--disable-yasm \
	--disable-asm \
	--disable-symver \
	--enable-gpl \
	--disable-ffmpeg \
	--disable-ffplay \
	--disable-ffprobe \
	--disable-ffserver \
	--cross-prefix=$CROSS_COMPILE \
	--enable-cross-compile \
	--sysroot=$SYSROOT \
	--enable-small \
	--extra-cflags="-Os -fpic $ADDI_CFLAGS" \
	--extra-ldflags="$ADDI_LDFLAGS" \
	$ADDITIONAL_CONFIGURE_FLAG
	make clean
	make
	make install
	echo "build ffmpeg for $ARCH finished"
}

#arm
ARCH=arm
CPU=arm
PREFIX=$(pwd)/android/$ARCH
TOOLCHAIN=$NDK_HOME/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64
CROSS_COMPILE=$TOOLCHAIN/bin/arm-linux-androideabi-
ADDI_CFLAGS="-marm"
SYSROOT=$NDK_HOME/platforms/$PLATFORM_VERSION/arch-$ARCH/
build

#x86
ARCH=x86
CPU=x86
PREFIX=$(pwd)/android/$ARCH
TOOLCHAIN=$NDK_HOME/toolchains/x86-4.9/prebuilt/linux-x86_64
CROSS_COMPILE=$TOOLCHAIN/bin/i686-linux-android-
ADDI_CFLAGS="-march=i686 -mtune=intel -mssse3 -mfpmath=sse -m32"
SYSROOT=$NDK_HOME/platforms/$PLATFORM_VERSION/arch-$ARCH/
build
```

### build_android_armeabi-v7a 针对armeabi-V7a平台
使用 neon 进行优化
-mfpu=neon

```text
#!/bin/bash
make clean
export NDK=/usr/work/ndk/android-ndk-r14b
export SYSROOT=$NDK/platforms/android-9/arch-arm/
export TOOLCHAIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64
export CPU=armeabi-v7a
export PREFIX=$(pwd)/android/$CPU
export ADDI_CFLAGS="-march=armv7-a -mfloat-abi=softfp -mfpu=neon"
export ADDI_LDFLAGS="-Wl,--fix-cortex-a8"

./configure --target-os=android \
--prefix=$PREFIX --arch=arm \
--enable-shared \
--disable-static \
--disable-yasm \
--disable-symver \
--enable-gpl \
--enable-ffmpeg \
--disable-ffplay \
--disable-ffprobe \
--disable-doc \
--disable-symver \
--cross-prefix=$TOOLCHAIN/bin/arm-linux-androideabi- \
--enable-cross-compile \
--sysroot=$SYSROOT \
--extra-cflags="-Os -fpic $ADDI_CFLAGS" \
--extra-ldflags="$ADDI_LDFLAGS" \
$ADDITIONAL_CONFIGURE_FLAG
make clean
make
make install
```

### androidStudio 集成ffmpeg
```text
1、将需要的so文件和头文件复制到工程中。
2、进行相关配置，以便生成适用于对应的cpu的最后的so。
3、CMakeLists.txt 配置示例
# cmake版本最低要求
cmake_minimum_required(VERSION 3.4.1)

add_library( # 将指定的cpp文件生成so文件
             wkplayer
             SHARED
             src/main/cpp/wkplayer.cpp )

# 导入 ffmpeg需要的头文件
include_directories(src/main/cpp/ffmpeg/include)

#添加libavcodec-57.so
# 指定so文件和引入方式等信息，最后生成 指定名字的 so
add_library( avcodec-57
             SHARED
             IMPORTED)
set_target_properties( avcodec-57
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libavcodec-57.so)

#添加libavdevice-57.so
add_library( avdevice-57
             SHARED
             IMPORTED)
set_target_properties( avdevice-57
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libavdevice-57.so)

add_library( avfilter-6
             SHARED
             IMPORTED)
set_target_properties( avfilter-6
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libavfilter-6.so)

add_library( avformat-57
             SHARED
             IMPORTED)
set_target_properties( avformat-57
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libavformat-57.so)

add_library( avutil-55
             SHARED
             IMPORTED)
set_target_properties( avutil-55
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libavutil-55.so)

add_library( swresample-2
             SHARED
             IMPORTED)
set_target_properties( swresample-2
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libswresample-2.so)

add_library( swscale-4
             SHARED
             IMPORTED)
set_target_properties( swscale-4
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libswscale-4.so)

add_library( postproc-54
             SHARED
             IMPORTED)
set_target_properties( postproc-54
                       PROPERTIES IMPORTED_LOCATION
                       ${CMAKE_SOURCE_DIR}/src/main/cpp/ffmpeg/armeabi/libpostproc-54.so)

find_library( # 调用系统日志库
              log-lib
              log )

target_link_libraries( # 将所有的库打包到一起
                        wkplayer
                        avcodec-57
                        avdevice-57
                        avfilter-6
                        avformat-57
                        avutil-55
                        swresample-2
                        swscale-4
                        postproc-54
                       ${log-lib} )

4、如果要给其他工程使用，提供 wkplayer.so 即可。不用提供ffmpeg的那些文件。
```

 
 
##  版本兼容
```text
#if LIBAVCODEC_VERSION_INT < AV_VERSION_INT(55, 28, 1)
#define av_frame_alloc  avcodec_alloc_frame
#endif
```

## ffmpeg 常用结构体
```text
AVIOContext // 用于从文件中读取数据，送去解析
AVStream | 存储每一个视频/音频流信息的结构体
AVOutputFormat |
AVInputFormat |
AVSampleFormat |音频采样率的格式
SwrContext |  一般用于音频重采样，比如采样率转换，声道转换。
AVFrame |  存储一帧的数据。例如yuv数据，是否关键帧、像素类型等
AVFormatContext |数据的封装格式、视音频流的个数、文件名、比特率、元数据等 
AVCodecContext |  记录编解码的一些信息。例如 采样格式、平均比特率、采样率等
AVCodec |  存储编解码器信息 。编解码器的名字、支持的帧率、类型（视频/音频/字幕）等
ANativeWindow| opengl工作的绘图画布本地窗口
SwsContext |  主要用于视频图像的转换
AVPicture | 
AVPacket | 存储压缩编码数据相关信息
AVBSFContext // The bitstream filter state
AVCodecParameters  //  描述一个解码后的流的属性。
AVRational  // 是一个有结构体，包含了分子和分母两个值
AVBitStreamFilter //
AVDictionaryEntry //键值对存储类
```
 

## ffmpeg常用函数
```text
ANativeWindow_fromSurface //创建播放窗口
av_register_all //注册所有组件
avformat_network_init //初始化网络库
avformat_network_deinit //
avformat_open_input |打开输入视频文件
av_find_best_stream |  查找音频流的 index
avformat_find_stream_info  //获取视频文件信息
av_format_inject_global_side_data ()//
avcodec_find_decoder |查找解码器
avcodec_open| 
avcodec_open2 | 打开解码器
avcodec_alloc_context3 | 
avcodec_parameters_to_context |
av_packet_alloc |
av_frame_alloc | 给AVFrame 分配内存
av_frame_free |  释放 AVFrame
av_packet_free |
avcodec_send_packet |送去解码
avcodec_receive_frame () // 从解码队列中取出一帧,AVFrame avFrame->data[0]   data[1]   data[2] ，y、u、v
avformat_alloc_context () // 创建解复用上下文
swr_alloc_set_opts |
swr_init |
swr_convert |重采样
sws_getContext |  初始化
sws_scale | 转换像素格式
sws_freeContext |
av_get_channel_layout_nb_channels |
av_samples_get_buffer_size |
av_packet_unref //
av_read_frame | 从输入文件读取一帧压缩数据 
avcodec_close() | 关闭解码器
avformat_close_input | 关闭输入视频文件
av_open_input_file |
av_find_stream_info|
av_find_input_format |
av_close_input_file |
avcodec_register_all|注册编解码器
avdevice_register_all |注册设备
avcodec_alloc_frame |
avpicture_get_size |
avpicture_fill |
img_convert |
avcodec_alloc_context( |
av_free_packet |
av_free |
avnew_steam |
av_write_frame |
dump_format |
avpicture_deinterlace |
ImgReSampleContext | 
avformat_open_input| 尝试打开连接 ，失败返回值 < 0
avformat_find_stream_info|  获取音频信息 并保存在AVFormatContext中
avcodec_open2 |  为各种结构体分配内存 和一些必要的检查
av_image_get_buffer_size | 计算缓冲区填充所需的大小
av_codec_next | 获取解码器
av_image_fill_arrays | 给像素数据分配空间
av_malloc |  分配内存
av_read_frame |   获得一帧视频的压缩数据 ，返回值< 0 代表失败
avcodec_decode_video | 
avcodec_decode_video2 |解码一帧压缩数据。 解码失败第三个参数返回值为0
avcodec_decode_video2
av_packet_ref(&dst,&packet) // 把第二个参数的内存数据拷贝到第一个参数的内存中
ANativeWindow_lock| 锁定 ANativeWindow , 返回值 小于0 代表失败
ANativeWindow_unlockAndPost | 释放锁定 ANativeWindow
ANativeWindow_release|
ANativeWindow_setBuffersGeometry | 设置ANativeWindow 的缓存大小,返回值< 0表示失败
ANativeWindow_fromSurface| surfaceview和ANativeWindow绑定
av_log_set_level( )  // 设置日志级别
av_log_get_level( ) //  获取日志级别
av_log_set_callback( )  //日志回调函数
avfilter_register_all( ) // 注册滤镜组件
av_frame_get_best_effort_timestamp () //获取pts
av_file_map () // 将指定文件映射到内存
av_rescale ()  // 转换为指定时间基下的时长
av_rescale_q () //

av_bsf_alloc( ) //
av_bsf_init() //
av_bsf_free( )  //
avcodec_parameters_copy() //
av_bsf_send_packet( )  // 送入过滤器
av_bsf_receive_packet( )  // 从过滤器中取出
avcodec_send_packet() // 送去解码
av_q2d () //   将时间转换为秒。转换前后的值基于同一时间基，仅仅是数值的表现形式不同而已。
av_usleep () // ffmpeg 里的休眠函数
avformat_free_context () // 释放context
av_bsf_get_by_name () // 根据名字获取 AVBitStreamFilter 
strcasecmp  () // 比较字符串
av_dict_set () // 设置键值对
av_dict_get () //读取键值对

av_get_default_channel_layout () // 获取通道类型
av_get_channel_layout_nb_channels () // 获取音频通道数
swr_alloc_set_opts () //  音频重采样设置
swr_init () //音频重采样初始化
swr_convert () // 音频重采样 执行
av_get_bytes_per_sample () // 获取每个sample中的字节数
av_dict_set () // 设置键值对
av_dict_get () // 获取键值对
int av_stristart (str, pfx, ptr)  //Return non-zero if pfx is a prefix of str independent of case
avformat_seek_file ()//从指定时间开始播放
av_read_pause () // Pause a network-based stream
```

## 常用常量和变量
```text
AV_PIX_FMT_YUV420P //  YUV420P格式
AV_NOPTS_VALUE  // 表示获取pts失败
AVERROR_EOF // 解码结束
AVMEDIA_TYPE_AUDIO  // 音频类型
AVMEDIA_TYPE_VIDEO // 视屏类型
AV_TIME_BASE  //时间基  1000000
stream_index //  用来匹配音视频流的索引
xx.interrupt_callback.callback  // 设置 读取超时回调
xx.interrupt_callback.opaque    //  常与interrupt_callback.callback一起写
xx.skip_initial_bytes //偏移量 Skip initial bytes when opening stream

// 健值存储和查找配置
AV_DICT_MATCH_CASE // 匹配大小写
AV_DICT_IGNORE_SUFFIX  //匹配时忽略后缀)
AV_DICT_DONT_STRDUP_KEY //不对key进行拷贝
AV_DICT_DONT_STRDUP_VAL //不对value进行拷贝
AV_DICT_DONT_OVERWRITE //不覆盖现有条目
AV_DICT_APPEND  //如果目已经存在，则value值直接拼接到之前的值的后面。
AV_DICT_MULTIKEY  //允许在字典中存储相等的key。

//标记常量
AVFMT_FLAG_GENPTS       // Generate missing pts even if it requires parsing future frames.
AVFMT_FLAG_IGNIDX       // Ignore index.
AVFMT_FLAG_NONBLOCK     // Do not block when reading packets from input.
AVFMT_FLAG_IGNDTS       // Ignore DTS on frames that contain both DTS & PTS
AVFMT_FLAG_NOFILLIN     // Do not infer any values from other values, just return what is stored in the container
AVFMT_FLAG_NOPARSE      // Do not use AVParsers, you also must set AVFMT_FLAG_NOFILLIN as the fillin code works on frames and no parsing -> no frames. Also seeking to frames can not work if parsing to find frame boundaries has been disabled
AVFMT_FLAG_NOBUFFER     // Do not buffer frames when possible
AVFMT_FLAG_CUSTOM_IO    // The caller has supplied a custom AVIOContext, don't avio_close() it.
AVFMT_FLAG_DISCARD_CORRUPT  // Discard frames marked corrupted
AVFMT_FLAG_FLUSH_PACKETS    // Flush the AVIOContext every packet.
AVFMT_FLAG_BITEXACT   // 
AVFMT_FLAG_MP4A_LATM    // Enable RTP MP4A-LATM payload
AVFMT_FLAG_SORT_DTS  //  try to interleave outputted packets by dts (using this flag can slow demuxing down)
AVFMT_FLAG_PRIV_OPT  //  Enable use of private options by delaying codec open (this could be made default once all code is converted)
AVFMT_FLAG_KEEP_SIDE_DATA    // Don't merge side data but keep it separate.
AVFMT_FLAG_FAST_SEEK   //  Enable fast, but inaccurate seeks for some formats

AVFMT_TS_DISCONT //
```
 




 
## 视音视频同步
### 为什么会出现音频不同步？
```text
理想状态下视音频是同步的，不过在现实场景下，由于机器性能、计算精度偏差等会导致音频和视频的同步性会慢慢地出现偏差。

DTS（Decoding Time Stamp）: 解码时间戳
PTS（Presentation Time Stamp）： 显示时间戳
音频、只有I帧和P帧的视频中 ，DTS和PTS是一样的。
当视频帧中有B帧的时候， DTS 和 PTS 是不一样的。 因为B帧跟前后帧都有关系，所以解码和显示顺序并不是一样的。
```

### 视音频同步方案
```text
1、将视频同步到音频上
以音频的播放速度为基准来同步视频。视频比音频播放慢了，加快其播放速度；快了，则延迟播放。
由于某些生物学的原理，人对声音的变化比较敏感，但是对视觉变化不太敏感。所以一般采用这种方案。
2、将音频同步到视频上
3、选择一个外部时钟为基准，视频和音频的播放速度都以该时钟为标准
```


### 视音频同步原理
```text
视音频的同步过程是一个动态过程，快则等待，慢则加快播放，在这样的过程中实现同步播放。
```

### 时间戳和时间基的理解
```text
Unix 时间戳
是一种时间表示方式，定义为从格林威治时间1970年01月01日00时00分00秒起至现在的总秒数。

ffmpeg中的时间戳概念跟 Unix的时间戳概念是有区别的，我的理解如下：
时间基可以理解为刻度或步长，例如时间基为1时，时间戳每加1就代表增加一秒钟，
时间基为2时，时间戳每加1就代表增加2秒，时间基为 1/25 时，时间戳每加1就代表增加 1/25 秒 。
如果以视频开始点开始计时 ，时间基为 2， 时间戳为2的时候 ，此时的播放时长就是 4秒 。

----------------------------------------------------------------
ffmpeg内部定义了一个时间基 AV_TIME_BASE

#define         AV_TIME_BASE   1000000
#define         AV_TIME_BASE_Q   (AVRational){1, AV_TIME_BASE}

timestamp(ffmpeg内部时间戳) = AV_TIME_BASE * time(秒)
time(秒) = AV_TIME_BASE_Q * timestamp(ffmpeg内部时间戳)
----------------------------------------------------------------

不同时间基下的时间戳转换
若 t代表时间 ，base1, base2 代表 时间基 ， stamps1 ,stamps2 代表时间戳
转换公式为  stamps1 =  (stamps2 * base2 ) / base1
在ffmpeg中可以直接使用内置函数

将在stream->time_base时间基下的时间戳pts 转为 AV_TIME_BASE时间戳下的时间戳time
int64_t time = av_rescale_q(pts, stream->time_base, AV_TIME_BASE_Q);
```




##  Windows 上使用 ffmpeg
### 安装ffmpeg
```text
1、 下载对应的版本 https://ffmpeg.zeranoe.com/builds/
2、 解压到本地
3、 将 xxx\ffmpeg\bin 配置到环境变量 。
如果不配置就不能全局使用命令。
4、测试是否成功
ffmpeg –version
```


### ffmpeg 常用命令
以下只是基本命令，高级应用请查阅文档
```text
ffmpeg -i [输入文件名] [参数选项] -f [格式] [输出文件]

// H264 转 mp4
ffmpeg -i test1.h264 -vcodec copy -f mp4 test.mp4

// .ts 转 mp4
ffmpeg -i test1.ts -vcodec copy -f mp4  test2.mp4

// mp4 转 flv
ffmpeg -i test1.mp4 -acodec copy -vcodec copy -f flv test1.flv

// 提取视频关键帧
ffmpeg -i test1.mp4 -vf select='eq(pict_type\,I)' -vsync 2 -s 1920*1080 -f image2 core-%03d.jpeg

// 视频截取
ffmpeg  -i ./test2.mp4 -vcodec copy -acodec copy -ss 00:00:08 -to 00:00:30 ./cutout1.mp4 -y

// 修改视频播放速度
// -an禁用声音  setpts范围[0.25, 4]，0.5是一倍速度
ffmpeg -i test2.mp4 -an -filter:v "setpts=0.5*PTS" out2.mp4

// 添加文字水印
ffmpeg -i test2.mp4 -vf "drawtext=fontfile=simhei.ttf: text='版权申明':x=200:y=100:fontsize=24:fontcolor=yellow:shadowy=2" out3.mp4

// 添加图片水印
ffmpeg -i test2.mp4 -i mark.png -filter_complex "[1:v]colorkey=0x000000:0.6:1.0[ckout];[0:v][ckout]overlay=x=W-w-10:y=0[out]" -map "[out]" -movflags faststart out4.mp4

// 视频转gif
// 从25秒开始，截取5秒 生成gif
ffmpeg -ss 25 -t 5 -i aa.mp4  -f gif aa.gif

// 提取音频为mp3
ffmpeg -i aa.mp4 -f mp3 a.mp3

// 提取视频 （删除声音）
ffmpeg -i aa.mp4 -vcodec copy -an  bb.avi
ffmpeg -i aa.mp4 -vcodec copy -an  bb.mp4

// 屏幕录制
// 全局录制
ffmpeg -f gdigrab -i desktop out.mp4
// 局部录制
ffmpeg -f gdigrab -framerate 25 -offset_x 10 -offset_y 20 -video_size 640x480 -i desktop out2.mp4

// 图片合成视频 （带声音）
未测试。
// 图片合成视频 （不带声音）
未测试。
//视频拼接
将两个视频 水平拼接 或者垂直拼接。 未测试。
```

## rtsp服务器Windows
window下搭建rtsp测试服务器可以用 live555 、vlc 等。 <br>
后来我发现用[EasyScreenLive](https://github.com/EasyDSS/EasyScreenLive)更简单。  [备用下载地址](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/201806271041_osChina_EasyScreenLive-Win-v1.0.18.0201.zip)


# OpenGLES 
OpenGL Shading Language 、Open Graphics Library

## 数据类型
```text
空类型  // void
标量 
向量 
矩阵 
采样器 
结构体 
数组 //只可以使用一维的数组
```

### 标量数据类型
```text
bool  // true 、false
int	  //和C一样，可以写为十进制（16）、八进制（020）或者十六进制（0x10）
float //
```

 
###  向量数据类型
```text
vec2  //  包含2个成分的向量。
bvec2	// 包含2个 boolean 变量
bvec3	// 包含3个 boolean 变量
bvec4	// 包含4个 boolean 变量
ivec2	// 包含2个 int 变量
ivec3	// 包含3个 int 变量
ivec4	// 包含4个 int 变量
```
 

### 矩阵数据类型
```text
 mat2x2 、mat2  // 2x2的浮点数矩阵类型
mat3x3 、mat3   // 3x3的浮点数矩阵类型
mat4x4 // 	4x4的浮点数矩阵
mat2x3 // 2列3行的浮点数矩阵
mat2x4 // 2列4行的浮点数矩阵
mat3x2 // 3列2行的浮点数矩阵
mat3x4 // 3列4行的浮点数矩阵
mat4x2 // 4列2行的浮点数矩阵
mat4x3 // 4列3行的浮点数矩阵
```


### 采样器数据类型
```text
sampler1D	// 用于内建的纹理函数中引用指定的1D纹理的句柄。<br>只可以作为一致变量或者函数参数使用
sampler2D	// 二维纹理句柄
sampler3D	// 三维纹理句柄
samplerCube	// cube map纹理句柄
sampler1DShadow	// 一维深度纹理句柄
sampler2DShadow	// 二维深度纹理句柄
```


## 修饰符 
```text
const	| 只读 ,不可修改
attribute	|只读的顶点数据。<br>可以是浮点数类型的标量，向量，或者矩阵。不可以是数组或则结构体
uniform	|一致变量。在着色器执行期间一致变量的值是不变的
varying	| 顶点着色器的输出。<br>可以是浮点数类型的标量，向量，矩阵。不能是数组或者结构体
centorid varying|	在没有多重采样的情况下，与varying是一样的意思。<br>在多重采样时，centorid varying在光栅化的图形内部进行求值而不是在片段中心的固定位置求值
invariant	|表示顶点着色器的输出和任何匹配片段着色器的输入
in  // 用在函数的参数中，表示这个参数是输入的
out	//用在函数的参数中，表示该参数是输出参数
inout //用在函数的参数，表示这个参数即是输入参数也是输出参数
```



顶点着色器 内置变量 | 类型| 说明
-|-|-
gl_Color|	vec4|	输入属性。表示顶点的主颜色
gl_SecondaryColor|	vec4|	输入属性。表示顶点的辅助颜色
gl_Normal	|vec3|	输入属性。表示顶点的法线值
gl_Vertex	|vec4	|输入属性。表示物体空间的顶点位置
gl_MultiTexCoordn	|vec4	|输入属性。表示顶点的第n个纹理的坐标
gl_FogCoord	|float|	输入属性。表示顶点的雾坐标
gl_Position	|vec4	| 是一个内置变量，用于指定顶点 ， 因为涉及到矩阵操作，所以用一个四维向量来赋值。
gl_ClipVertex	|vec4	|输出坐标。用于用户裁剪平面的裁剪
gl_PointSize	|float|	是内置变量 ,用于指定点的大小
gl_FrontColor	|vec4|	正面的主颜色的varying输出
gl_BackColor	|vec4	|背面主颜色的varying输出
gl_FrontSecondaryColor	|vec4	|正面的辅助颜色的varying输出
gl_BackSecondaryColor	|vec4	|背面的辅助颜色的varying输出
gl_TexCoord[]	|vec4	|纹理坐标的数组varying输出
gl_FogFragCoord	|float|	雾坐标的varying输出


片段着色器 内置变量 | 类型| 说明
-|-|-
gl_Color	|vec4	|包含主颜色的插值只读输入
gl_SecondaryColor	|vec4	|包含辅助颜色的插值只读输入
gl_TexCoord[]	|vec4	|包含纹理坐标数组的插值只读输入
gl_FogFragCoord|	float	|包含雾坐标的插值只读输入
gl_FragCoord|	vec4	|只读输入。窗口的x,y,z和1/w
gl_FrontFacing|	bool	|只读输入。<br>如果是窗口正面图元的一部分，则这个值为true
gl_PointCoord|	vec2|	点精灵的二维空间坐标范围在(0.0, 0.0)到(1.0, 1.0)之间。<br>仅用于点图元和点精灵开启的情况下。
gl_FragData[]	|vec4	|使用glDrawBuffers输出的数据数组。不能与gl_FragColor结合使用。
gl_FragColor|	vec4|	片元颜色,是内置变量， 四个分量（r, g, b, a）
gl_FragDepth	|float|	输出的深度用于随后的像素操作。<br>如果这个值没有被写，则使用固定功能管线的深度值代替


## 常用内置函数 
```text
all |在所有为真的时候，返回为真；
any |在任一变量为真的时候，返回为真；
dot |向量点乘
cross |向量的叉积
ceil(x) |向上取整 如：ceil(3.5) 值为4
floor(x) |向下取整 如：floor(3.5) 值为3
pow(x,y) |求x的y次方
exp(x) |自然指数e的x次方
exp2(x) |2的x次方
transpose |矩阵求转置矩阵
determinant |求行列式
inverse |矩阵求逆矩阵
distance |计算两点的距离
length |求解向量的长度
normalize |对一个向量进行标准化
clamp |将一个值截取到两个值之间
matrixCompMult |在两个矩阵之间执行一个逐分量的操作
outerProduct |提取两个向量的外积。<br>就是将一个n*1的向量与1* m的向量相乘，得到一个n*m的矩阵
dot ( )  // 点乘 返回两个单位向量之间夹角的cos值
cross ( )  // 叉乘
texture2D ( )  // 用于纹理采样
normalize  ( )  //:对⼀一个向量量规格化
clamp  ( )  //将⼀个向量固定在一个最小值和最大值之间
pow() //幂函数
exp()  //  log() 指数函数 对数函数
sqrt()   // 平方根
max()   // min()  // 最大值 最小值
ceil() floor()  // 取大于实参的最小整数，取小于实参的最大整数
sin() cos() tan()   // 三角函数
asin() acos() atan()   // 反三角函数
sinh() cosh() tanh()   // 双曲正弦 双曲余弦 双曲正切
asinh() acosh() atanh()   // 反双曲正弦 反双曲余弦 反双曲正切
length ()   // 向量长度
distance ()   // 两个向量的距离
matrixCompMult ()   // 矩阵对应元素分别相乘
transpose () determinant() inverse()   // 矩阵转置 行列式 逆
lessThan () greaterThan() equal()  //  小于 大于 等于 
```

精度级别 | 说明
-|-
lowp|
mediump|  
highp|precision highp float; <br> 定义在 float 类型默认使用 highp 级别的精度


## 关键字
```text
precision  // 系统关键字 ，用来指定数据类型的精度
const	  // 变量类型，编译过程常量，或者函数的只读参数。 只读 ,不可修改。
attribute // 变量类型 ， 表示只读的顶点数据，只用在顶点着色器中。用于连接顶点着色器和 OpenGL ES 。
uniform  // 变量类型 ， 链接在一起的顶点着色器和片段着色器共享同一个统一变量空间
varying // 变量类型 ，  顶点着色器的输出。  用于连接顶点着色器和片段着的输出。 
```

## 其他
```text
samplerExternalOES  //安卓上我们只能用samplerExternalOES类型的纹理去接收摄像头的画面
#extension GL_OES_EGL_image_external : require  //  android中使用samplerExternalOES  ， 需要开启
```

 
##  专业名词

名词 | 作用
-|-
OpenGL|一个跨平台的图形API，提供了软件操作３Ｄ图形硬件的接口。<br>OpenGL 的基本形状是三角形。
OpenGLES|一个专用于嵌入式设备的阉割版的OpenGL。<br> [Android 1.0 , +∞) 支持 OpenGL ES 1.0 <br> [Android 2.2 , +∞)  支持 OpenGL ES 2.0 <br> [Android 4.3 , +∞) 支持 OpenGL ES 3.0 <br> [Android 5.0 , +∞) 支持OpenGL ES 3.1
GLSL <br>(OpenGL Shader Language)|着色器语言。面向过程。
顶点着色器 Vertex shader| 把顶点在虚拟空间中的三维坐标变换为在屏幕上的二维坐标。<br> 并带有用于z-buffer的深度信息。
片元着色器 Fragment shader|针对每个片元(像素)，用于确定每个片元的颜色。
EGL<br>(Embedded Graphics Library) |连接OpenGL ES和本地窗口系统的接口，屏蔽不同平台上的区别。从而实现跨平台。
渲染管线|指的是一堆原始图形数据途经一个输送管道，期间经过各种变化处理最终出现在屏幕的过程
投影矩阵<br>Projection Matrix |在OpenGL中，投影矩阵用来改变场景在屏幕上的显示方式（近大远小、平行投影等）
正交投影|投影线是平行的。物体不会随距离观测点的位置而大小发生变化。
投影 | 让3D世界呈现在2D世界的过程。
透视投影|模仿人眼观察3D世界的规律，让物体近大远小。
光栅化|即像素化。<br>把点、线、三角形映射到屏幕上的像素点的过程。
光照| 1、环境光  <br> 2、镜面光 <br> 3、散射光
VBO|顶点缓冲对象（Vertex Buffer Objects，VBO）<br>保存了一个模型的顶点属性信息
VAO|顶点数组对象（Vertex Arrary Object，VAO）<br>VAO相当于是对很多个VBO的引用
EBO|索引缓冲对象（Element Buffer Object，EBO）<br>存储顶点位置的索引
FBO|  Frame Buffer Object  帧缓冲对象
纹理|可以将纹理看做应用在物体表面的像素颜色。
纹理映射|将2D的纹理映射到3D场景中的立体物体上。
贴图|
阴影|
粒子|
混合与雾|
标志板|
天空盒|
天空穹|
相机UP方向|理解为相机顶端指向的方向。斜着、倒着之类的
.STL文件|3D模型文件
.sh|着色器语言文件
.obj|3D模型文件
.mtl|材质、贴图文件


##  坐标系类型

类型 | 特点
-|-
局部坐标系(Local Space) / (Object Space)| 以自身为参照点
世界坐标系(World Space)|[ -1 , 1 ]标。<br>
相机坐标系(View Space) / (Eye Space)|相机所在的位置为原点。XYZ轴的方向 根据情况而定。
裁剪坐标系(Clip Space)| ？
屏幕坐标系(Screen Space)|就是呈现在屏幕上的大小
纹理坐标系|[ -1 , 1 ]
```text
注：纹理坐标的y轴的方向和图片的Y轴方向相反。所以是要进行y轴反转的 。？？
```

![](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/201804171454_osChina_坐标系.png)




## GLES20 常用变量
[GLES20](https://developer.android.google.cn/reference/android/opengl/GLES20.html) 常用变量 | 意义
-|-
GL_DEPTH_BUFFER_BIT | 清空 深度缓冲区
GL_COLOR_BUFFER_BIT| 清空 颜色缓冲区
GL_ACCUM_BUFFER_BIT|清空 累积缓冲区
GL_STENCIL_BUFFER_BIT| 清空模板缓冲区
GL_FLOAT|
GL_TRIANGLES|每三个顶点绘制一个三角形
GL_TRIANGLE_STRIP| 绘制三角形的一种顺序
GL_TRIANGLE_FAN|绘制三角形的一种顺序
GL_TEXTURE0|
GL_TEXTURE_2D|
GL_UNSIGNED_SHORT|
GL_VERTEX_SHADER|
GL_FRAGMENT_SHADER|
GL_LINK_STATUS|
GL_TRUE|
GL_COMPILE_STATUS|
GLES20 常用函数 | 意义
glGetUniformLocation ( int program,String name) |根据name获取uniform类型变量的id
glGetAttribLocation( int program,  String name)| 根据name获取attribute类型变量的id
glUseProgram|将程序加入到OpenGLES环境
glEnableVertexAttribArray|启用顶点坐标数组<br>启动顶点纹理数组
glVertexAttribPointer|准备顶点位置数据
glDrawArrays|图形绘制
glClear|清除缓冲区
glCreateProgram|创建一个空的OpenGLES程序
glUniformMatrix4fv|将最终变换矩阵传入shader程序
glDisableVertexAttribArray|禁止顶点数组的句柄
glDrawElements|索引法绘制
glUniform4fv|设置绘制三角形的颜色
glUniform1i|
glUniform1f|
glBindTexture|绑定纹理
glActiveTexture|选择活动纹理单元
glShaderSource|将资源加入到着色器中
glCompileShader|编译着色器
glGetShaderiv|
glDeleteShader|
glCreateShader|根据type创建着色器
glAttachShader|将着色器加入到程序
glLinkProgram|连接到着色器程序
glGetProgramiv|
glDeleteProgram|
glClearColor|清屏时，使用该颜色填充整个屏幕
glViewport|确定绘制区域
glReadPixels | 获取正在显示的像素
glGenTextures|生成纹理
glTexParameterf|纹理过滤函数
texImage2D|生成2D纹理

## EGL10 常用函数
[EGL10](https://developer.android.google.cn/reference/javax/microedition/khronos/egl/EGL10.html) 常用函数 | 意义
-|-
glMatrixMode|使用的矩阵类型
glFrustumf|三维用二维显示出来需要指定的一些参数
glLoadIdentity|设置为单位矩阵
glDepthFunc|深度比较
GL_MODELVIEW|观察矩阵
GL_PROJECTION|投影矩阵
glClearDepthf|给深度缓存设定默认值
glHint|希望进行最好的透视修正
glEnableClientState|启用顶点数组等
glShadeModel|设置着色器模式
glVertexPointer|指向顶点数组
glColorPointer|指向颜色数组的指针
glDrawArrays|绘制数组里面所有点构成的各个三角片

[EGLConfig](https://developer.android.google.cn/reference/javax/microedition/khronos/egl/EGLConfig.html)常用函数 | 意义
-|-
|
|


Matrix 常用函数 | 意义|参数
-|-|-
orthoM|正交投影|orthoM <br>(<br>float[] m,//接收正交投影的变换矩阵<br>int mOffset,//变换矩阵的起始位置（偏移量）<br>float left,//相对观察点近面的左边距<br>float right,//相对观察点近面的右边距<br>float bottom,//相对观察点近面的下边距<br>float top,//相对观察点近面的上边距<br>float near,//相对观察点近面距离<br>float far//相对观察点远面距离<br>)
frustumM|透视投影|Matrix.frustumM <br>(<br>float[] m,//接收透视投影的变换矩阵<br>int mOffset,//变换矩阵的起始位置（偏移量）<br>float left,//相对观察点近面的左边距<br>float right,//相对观察点近面的右边距<br>float bottom, //相对观察点近面的下边距<br>float top, //相对观察点近面的上边距<br>float near,//相对观察点近面距离<br>float far //相对观察点远面距离<br>)
setLookAtM|设置观察矩阵(相机位置)|setLookAtM<br> (<br>float[] rm,//接收相机变换矩阵 <br>int rmOffset,//变换矩阵的起始位置（偏移量）<br> float eyeX,float eyeY, float eyeZ, //相机位置<br>float centerX,float centerY,float centerZ, //观测点位置<br>float upX,float upY,float upZ//up向量在xyz上的分量 <br>)
multiplyMM|矩阵相乘|multiplyMM<br>(<br>float[] result,  //接收相乘结果<br> int resultOffset,//接收矩阵的起始位置（偏移量）<br> float[] lhs, //左矩阵<br>  int lhsOffset, //左矩阵的起始位置（偏移量）<br> float[] rhs, //右矩阵<br>  int rhsOffset//右矩阵的起始位置（偏移量）<br>)


## 常用函数
```text
glFlush ()  //  强制刷新缓冲，保证绘图命令将被执行
eglSwapBuffers () ;// 提交当前的帧
glDisable (GL_DITHER); //  取消抖动
glEnable (GL_CULL_FACE)  // 开启剔除操作效果
glDeleteTextures () // 删除纹理
glDeleteFramebuffers () //
eglGetDisplay() // 获取EGLDisplay 
eglInitialize() // 初始化EGL,并回传支持的版本
eglChooseConfig () // 选择config
eglCreateContext () //
eglGetConfigAttrib () //    
ANativeWindow_setBuffersGeometry () // 设置 ANativeWindow 绘制窗口属性
ANativeWindow_release () //
eglCreateWindowSurface () //
eglMakeCurrent () //  把EGLContext和EGLSurface关联起来 
eglDestroySurface () //
eglDestroyContext () //
glBindFramebuffer () // 绑定一个命名的帧缓冲区对象
glGenFramebuffers () // 生成framebuffer 的id
glTexImage2D () // 生成一个2D纹理
glFramebufferTexture2D () // 绑定2D纹理
glTexParameteri() // 图象从纹理图象空间映射到帧缓冲图象空间 
glUniform1f () //为当前程序对象指定Uniform变量的值。
void glUniform1f (  );
void glUniform2f ( );
void glUniform3f ( );
void glUniform4f ( );
void glUniform1i (  );  // 保证每个uniform采样器对应着正确的纹理单元。
void glUniform2i (   );
void glUniform3i (  );
void glUniform4i (  );
void glUniform1fv (  );
void glUniform2fv ( );
void glUniform3fv (  );
void glUniform4fv ();
void glUniform1iv ( );
void glUniform2iv ( );
void glUniform3iv ( );
void glUniform4iv ( );
void glUniformMatrix2fv ( );
void glUniformMatrix3fv (  );
void glUniformMatrix4fv (  );

EGLBoolean // EGL_TRUE =1, EGL_FALSE=0
EGLint // int 数据类型
EGLDisplay // 系统显示 ID 或句柄，可以理解为一个前端的显示窗口
EGLConfig //Surface的EGL配置，可以理解为绘制目标framebuffer的配置属性
EGLSurface //系统窗口或 frame buffer 句柄 ，可以理解为一个后端的渲染目标窗口。
EGLContext //OpenGL ES 图形上下文，它代表了OpenGL状态机；如果没有它，OpenGL指令就没有执行的环境。
GL_FALSE  //
EGL_NO_DISPLAY //
EGL_NO_SURFACE //
EGL_NO_CONTEXT //
EGL_DEFAULT_DISPLAY  //
EGL_NO_DISPLAY  //
EGL_NO_SURFACE //
EGL_RED_SIZE    //指定RGB中的R大小（bits）
EGL_GREEN_SIZE  //指定G大小
EGL_BLUE_SIZE    //指定B大小
EGL_RENDERABLE_TYPE 
EGL_OPENGL_ES3_BIT_KHR // 
EGL_SURFACE_TYPE // 
EGL_WINDOW_BIT   // 
EGL_NONE //
EGL_CONTEXT_CLIENT_VERSION // context 版本
EGL_NO_CONTEXT  // 表示不向其它的context共享资源
EGL_NATIVE_VISUAL_ID  // 显示设备的原始图像编号
NO_TEXTURE // 
ON_DRAWN //
GL_FRAMEBUFFER  //
GL_RGBA //
GL_UNSIGNED_BYTE //
GL_TEXTURE_MIN_FILTER   // 缩小过滤
GL_TEXTURE_MAG_FILTER   // 放大过滤
GL_TEXTURE_WRAP_S   // S方向上的贴图模式
GL_TEXTURE_WRAP_T   //  T 方向上的贴图模式
GL_LINEAR   //
GL_NEAREST   //
GL_CLAMP_TO_EDGE    //
GL_COLOR_ATTACHMENT0  //
GLES20.GL_REPEAT //
GLES20.GL_LINEAR // 
```

## 常用类与常量变量
```text
GLuint  //  无符号四字节整型，包含数值从0 到 4,294,967,295
GL_DEPTH_TEST  //  深度比较和更新深度缓冲
```



## 补充细节
```text
1、在OpenGL中提到的浮点数都是指float类型。
2、在OpenGL中，数组是row-major (行优先)的。
3、绘制三角形取顶点的顺序 是逆时针的。
4、OpenGL中使用的是列向量
```


## 索引绘制
![](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/201803291656_osChina_索引绘制.png)
这个多边形由2个三角形组成。 蓝色和绿色三角形的 组成顶点依次是 (V0 , V1 ,V2) ,( V3 ,V4, V5) 。<br> 注意：绘制三角形 是 逆时针绘制的规则。<br>

```text
//顶点数组如下。 有重叠的点
[ (0,0), (2,0), (1,2), (1,2), (2,0), (3,2)]
//对应的索引数组如下。 分别代表顶点数组中的 第 0,1,2,2,1,5个数据
[ 0, 1,2,  2,1,5]
```

&nbsp;&nbsp;

##   长宽比例变形问题
opengl的坐标系是 1:1的比例 ，然而手机等设备是 16:9 等长方形比例。所以一个正方形直接显示在屏幕后就是一个长方形。 此时，我们可以通过 计算设备的长宽比，然后根据比例重新计算在opengl中的坐标 ，最终可以获得正方形效果。 但是，如果绘制的图形比较复杂，我们通过这种方式去计算，会很繁琐。此时，我们可以通过 变换矩阵 ，让opengl库帮我们完成这些工作。  矩阵转换的推导和具体转换原理，以后再详细展示，此处略过。



## 球面绘制
将图片绘制到球面体 ，形成 全景效果 。
??????????????
而我们的视角在是在球的中心点，这样就可以向四面八方去观看了。



## android 配置OpenGL ES 环境
```text
// 使用 opengl es 版本
<uses-feature android:glEsVersion="0x00020000" android:required="true" />
//使用纹理压缩
<supports-gl-texture android:name="GL_OES_compressed_ETC1_RGB8_texture" />
<supports-gl-texture android:name="GL_OES_compressed_paletted_texture" />
```

```text
// EGL 版本
glSurfaceView.setEGLContextClientVersion(2);
glSurfaceView.setRenderer(new MyGLRenderer(this));
// 渲染模式
glSurfaceView.setRenderMode(GLSurfaceView.RENDERMODE_CONTINUOUSLY);

/** 是否支持 opengl2.0 + */
private boolean checkSupported() {
 ActivityManager activityManager = (ActivityManager) getSystemService(ACTIVITY_SERVICE);
 if (activityManager != null) {
     ConfigurationInfo configurationInfo = activityManager.getDeviceConfigurationInfo();
     boolean supportsEs2 = configurationInfo.reqGlEsVersion >= 0x2000;
     boolean isEmulator = Build.VERSION.SDK_INT > Build.VERSION_CODES.ICE_CREAM_SANDWICH_MR1
             && (Build.FINGERPRINT.startsWith("generic")
             || Build.FINGERPRINT.startsWith("unknown")
             || Build.MODEL.contains("google_sdk")
             || Build.MODEL.contains("Emulator")
             || Build.MODEL.contains("Android SDK built for x86"));

     supportsEs2 = supportsEs2 || isEmulator;
     return supportsEs2 ;
 }else{
     return false;
 }
}
```






```text
final float mTriangleArray[] = {
       -1.0f,1.0f,1.0f,
       -1.0f,-1.0f,1.0f,
       1.0f,-1.0f,1.0f
};
public void test(float[] data)
{
 // 分配本地内存块 , 一个float占4个字节
 ByteBuffer bb = ByteBuffer.allocateDirect(data.length * 4);
 //以本机字节顺序来修改此缓冲区的字节顺序
 bb.order(ByteOrder.nativeOrder());
 FloatBuffer mTriangleBuffer = bb.asFloatBuffer();
 //将给定float[]数据从当前位置开始，依次写入此缓冲区
 mTriangleBuffer.put(mTriangleArray);
 //设置此缓冲区的位置。
 mTriangleBuffer.position(0);
}
```


## openglES示例

### 全景

全景视频类型 | 特点
-|-
Sphere|
Skybox|
Cubemap|
Cylinder|

 


### 彩色三角形
```text
public class MyRenderer implements GLSurfaceView.Renderer {
    private Context context ;
    private float[] mViewMatrix=new float[16];
    private float[] mProjectMatrix=new float[16];
    private float[] mMVPMatrix=new float[16];
    private int mProgram;
    private FloatBuffer vertexBuffer;
    private FloatBuffer colorBuffer;

    /**顶点坐标*/
    private float triangleCoords[] = {
   0.5f,  0.5f, 0.0f,
   -0.5f, -0.5f, 0.0f,
   0.5f, -0.5f, 0.0f
    };

    float color[] = {
   0.0f, 1.0f, 0.0f, 1.0f ,
   1.0f, 0.0f, 0.0f, 1.0f,
   0.0f, 0.0f, 1.0f, 1.0f
    };

    public MyRenderer( Context context){
        this.context = context;
        init();
    }

    private void init()  {
        ByteBuffer bb = ByteBuffer.allocateDirect(  triangleCoords.length * 4);
        bb.order(ByteOrder.nativeOrder());
        vertexBuffer = bb.asFloatBuffer();
        vertexBuffer.put(triangleCoords);
        vertexBuffer.position(0);

        ByteBuffer dd = ByteBuffer.allocateDirect(  color.length * 4);
        dd.order(ByteOrder.nativeOrder());
        colorBuffer = dd.asFloatBuffer();
        colorBuffer.put(color);
        colorBuffer.position(0);
    }

    @Override
    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
        String vertexShaderCode = MyAssetsTool.loadFromAssetsFile("vertex.sh" ,context);
        String fragmentShaderCode = MyAssetsTool.loadFromAssetsFile("fragment.sh" ,context);
        int vertexShader = loadShader(GLES20.GL_VERTEX_SHADER,  vertexShaderCode);
        int fragmentShader = loadShader(GLES20.GL_FRAGMENT_SHADER,  fragmentShaderCode);

        mProgram = GLES20.glCreateProgram();
        GLES20.glAttachShader(mProgram, vertexShader);
        GLES20.glAttachShader(mProgram, fragmentShader);
        GLES20.glLinkProgram(mProgram);
    }

    @Override
    public void onSurfaceChanged(GL10 gl, int width, int height) {
        //计算宽高比
        float ratio=(float)width/height;
        Matrix.frustumM(mProjectMatrix, 0, -ratio, ratio, -1, 1, 3, 7);
        Matrix.setLookAtM(mViewMatrix, 0, 0, 0, 7.0f, 0f, 0f, 0f, 0f, 1.0f, 0.0f);
        Matrix.multiplyMM(mMVPMatrix,0,mProjectMatrix,0,mViewMatrix,0);
    }

    @Override
    public void onDrawFrame(GL10 gl) {
        GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT | GLES20.GL_DEPTH_BUFFER_BIT);
        GLES20.glUseProgram(mProgram);

        int mMatrixHandler= GLES20.glGetUniformLocation(mProgram,"vMatrix");
        GLES20.glUniformMatrix4fv(mMatrixHandler,1,false,mMVPMatrix,0);

        // 坐标
        int mPositionHandle = GLES20.glGetAttribLocation(mProgram, "vPosition");
        GLES20.glEnableVertexAttribArray(mPositionHandle);
        GLES20.glVertexAttribPointer(mPositionHandle, 3,  GLES20.GL_FLOAT, false, 3*4, vertexBuffer);

        // 颜色
        int mColorHandle = GLES20.glGetAttribLocation(mProgram, "aColor");
        GLES20.glEnableVertexAttribArray(mColorHandle);
        GLES20.glVertexAttribPointer(mColorHandle,4,  GLES20.GL_FLOAT,false, 0,colorBuffer);

        // 绘制
        GLES20.glDrawArrays(GLES20.GL_TRIANGLES, 0, 3);
        GLES20.glDisableVertexAttribArray(mPositionHandle);

    }

    private int loadShader(int type, String shaderCode){
        int shader = GLES20.glCreateShader(type);
        GLES20.glShaderSource(shader, shaderCode);
        GLES20.glCompileShader(shader);
        return shader;
    }

}

/**
 * 从 assets 目录中加载 文件 转为 String
 * @param fname 基于assets的绝对路径
 * @param context context
 * @return String
 */
public static String loadFromAssetsFile(String fname, Context context){
    StringBuilder result=new StringBuilder();
    try{
        InputStream is=context.getResources().getAssets().open(fname);
        int ch;
        byte[] buffer=new byte[1024];
        while (-1!=(ch=is.read(buffer))){
            result.append(new String(buffer,0,ch));
        }
    }catch (Exception e){
        return null;
    }
    return result.toString().replaceAll("\\r\\n","\n");
}

// fragment.sh
precision mediump float;
varying vec4 vColor;
void main() {
 gl_FragColor = vColor;
}

// vertex.sh
attribute vec4 vPosition;
uniform mat4 vMatrix;
varying  vec4 vColor;
attribute vec4 aColor;
void main() {
    gl_Position = vMatrix * vPosition;
    vColor= aColor;
}
```

### 正方形
可以看做由两个三角形组成。

```text
public class MyRenderer implements GLSurfaceView.Renderer {
    private Context context ;
    private float[] mViewMatrix=new float[16];
    private float[] mProjectMatrix=new float[16];
    private float[] mMVPMatrix=new float[16];
    private int mProgram;
    /**顶点坐标数据*/
    private FloatBuffer vertexBuffer;
    /**顶点索引数据*/
    private ShortBuffer indexBuffer;
    /**顶点坐标*/
    private float triangleCoords[] = {
            -0.5f,  0.5f, 0.0f,
            -0.5f, -0.5f, 0.0f,
            0.5f, -0.5f, 0.0f,
            0.5f,  0.5f, 0.0f
    };
    /**索引法*/
    private short index[]={  0,1,2,0,2,3 };

    float color[] = {
       0.0f, 1.0f, 0.0f, 1.0f ,
       1.0f, 1.0f, 0.0f, 1.0f,
       0.0f, 0.0f, 1.0f, 1.0f
    };

    public MyRenderer( Context context){
        this.context = context;
        init();
    }
    private void init()  {
        ByteBuffer bb = ByteBuffer.allocateDirect(  triangleCoords.length * 4);
        bb.order(ByteOrder.nativeOrder());
        vertexBuffer = bb.asFloatBuffer();
        vertexBuffer.put(triangleCoords);
        vertexBuffer.position(0);

        ByteBuffer dd = ByteBuffer.allocateDirect(  color.length * 4);
        dd.order(ByteOrder.nativeOrder());
        indexBuffer=dd.asShortBuffer();
        indexBuffer.put(index);
        indexBuffer.position(0);
    }

    @Override
    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
        String vertexShaderCode = MyAssetsTool.loadFromAssetsFile("vertex.sh" ,context);
        String fragmentShaderCode = MyAssetsTool.loadFromAssetsFile("fragment.sh" ,context);
        int vertexShader = loadShader(GLES20.GL_VERTEX_SHADER,  vertexShaderCode);
        int fragmentShader = loadShader(GLES20.GL_FRAGMENT_SHADER,  fragmentShaderCode);
        mProgram = GLES20.glCreateProgram();
        GLES20.glAttachShader(mProgram, vertexShader);
        GLES20.glAttachShader(mProgram, fragmentShader);
        GLES20.glLinkProgram(mProgram);
    }

    @Override
    public void onSurfaceChanged(GL10 gl, int width, int height) {
        //计算宽高比
        float ratio=(float)width/height;
        Matrix.frustumM(mProjectMatrix, 0, -ratio, ratio, -1, 1, 3, 7);
        Matrix.setLookAtM(mViewMatrix, 0, 0, 0, 7.0f, 0f, 0f, 0f, 0f, 1.0f, 0.0f);
        Matrix.multiplyMM(mMVPMatrix,0,mProjectMatrix,0,mViewMatrix,0);
    }

    @Override
    public void onDrawFrame(GL10 gl) {
        GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT | GLES20.GL_DEPTH_BUFFER_BIT);
        GLES20.glUseProgram(mProgram);

        int mMatrixHandler= GLES20.glGetUniformLocation(mProgram,"vMatrix");
        GLES20.glUniformMatrix4fv(mMatrixHandler,1,false,mMVPMatrix,0);

        int mPositionHandle = GLES20.glGetAttribLocation(mProgram, "vPosition");
        GLES20.glEnableVertexAttribArray(mPositionHandle);

        GLES20.glVertexAttribPointer(mPositionHandle, 3, GLES20.GL_FLOAT, false,  3*4, vertexBuffer);
        int mColorHandle = GLES20.glGetUniformLocation(mProgram, "vColor");
        GLES20.glUniform4fv(mColorHandle, 1, color, 0);

        GLES20.glDrawElements(GLES20.GL_TRIANGLES,index.length, GLES20.GL_UNSIGNED_SHORT,indexBuffer);
        GLES20.glDisableVertexAttribArray(mPositionHandle);

    }

    private int loadShader(int type, String shaderCode){
        int shader = GLES20.glCreateShader(type);
        GLES20.glShaderSource(shader, shaderCode);
        GLES20.glCompileShader(shader);
        return shader;
    }

}

```

```text
attribute vec4 vPosition;
uniform mat4 vMatrix;
void main() {
    gl_Position = vMatrix * vPosition;
}
```

```text
precision mediump float;
uniform vec4 vColor;
void main() {
 //片元颜色
 gl_FragColor = vColor;
}
```


### 圆
可以把圆形看成一个正多边形，边越多，圆越平滑。
![](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/201804171623_osChina_画圆.png)  <br>
A点的横坐标为：   r x cos θ   <br>
A点的纵坐标为：   r x sin θ
```text
//设置圆心坐标
data.add(0.0f);
data.add(0.0f);
data.add(0f);
float angDegSpan=360f/count;
for(float i=0;i<360+angDegSpan;i+=angDegSpan)
{
    data.add((float) (r*Math.sin(i*Math.PI/180f)));
    data.add((float)(r*Math.cos(i*Math.PI/180f)));
    data.add(0f);
}
```

```text
public class MyRenderer implements GLSurfaceView.Renderer {
    private Context context;
    private FloatBuffer vertexData;
    /**圆的半径*/
    private float r = 0.5f;
    /**将圆分成n份*/
    private int count = 100;
    private int program;
    private int uColorLocation;
    private int aPositionLocation;
    private final float[] projectionMatrix = new float[16];
    private int uMatrixLocation;

    public MyRenderer(Context context){
        this.context = context;
        init();
    }

    private void init(){
        ArrayList<Float> data=new ArrayList<>();
        //设置圆心坐标
        data.add(0.0f);
        data.add(0.0f);
        data.add(0f);
        float angDegSpan=360f/count;
        for(float i=0;i<360+angDegSpan;i+=angDegSpan){
            data.add((float) (r*Math.sin(i*Math.PI/180f)));
            data.add((float)(r*Math.cos(i*Math.PI/180f)));
            data.add(0f);
        }
        float[] f=new float[data.size()];
        for (int i=0;i<f.length;i++){
            f[i]=data.get(i);
        }
        ByteBuffer bb = ByteBuffer.allocateDirect(  f.length * 4);
        bb.order(ByteOrder.nativeOrder());
        vertexData = bb.asFloatBuffer();
        vertexData.put(f);
        vertexData.position(0);
    }

    @Override
    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
        gl.glClearColor(1.0f, 1.0f, 1.0f, 1.0f);

        String vertexShaderCode = MyAssetsTool.loadFromAssetsFile("vertex.sh" ,context);
        String fragmentShaderCode = MyAssetsTool.loadFromAssetsFile("fragment.sh" ,context);
        int vertexShader = loadShader(GLES20.GL_VERTEX_SHADER,  vertexShaderCode);
        int fragmentShader = loadShader(GLES20.GL_FRAGMENT_SHADER,  fragmentShaderCode);
        program = GLES20.glCreateProgram();
        GLES20.glAttachShader(program, vertexShader);
        GLES20.glAttachShader(program, fragmentShader);
        GLES20.glLinkProgram(program);
        GLES20.glUseProgram(program);

        aPositionLocation = GLES20.glGetAttribLocation(program,  "a_Position");
        uMatrixLocation = GLES20.glGetUniformLocation(program, "u_Matrix");
        GLES20.glVertexAttribPointer(aPositionLocation, 3,GLES20.GL_FLOAT, false, 0, vertexData);
        GLES20.glEnableVertexAttribArray(aPositionLocation);
        uColorLocation = GLES20.glGetUniformLocation(program, "vColor");
    }
    @Override
    public void onSurfaceChanged(GL10 gl, int width, int height) {
        gl.glViewport(0,0,width,height);
        final float aspectRatio = width > height ?  (float) width / (float) height : (float) height / (float) width;
        if(width > height){
            Matrix.orthoM(projectionMatrix, 0, -aspectRatio, aspectRatio, -1f, 1f, -1f, 1f);
        }else{
            Matrix.orthoM(projectionMatrix, 0, -1f, 1f, -aspectRatio, aspectRatio, -1f, 1f);
        }
    }
    @Override
    public void onDrawFrame(GL10 gl) {
        GLES20.glClearColor(0,0,0,0);
        GLES20.glUniform4f(uColorLocation, 0.0f, 0.0f, 1.0f, 1.0f);
        GLES20.glUniformMatrix4fv(uMatrixLocation, 1, false, projectionMatrix,0);
        GLES20.glDrawArrays(GLES20.GL_TRIANGLE_FAN, 0, count +2);
    }
    private int loadShader(int type, String shaderCode){
        int shader = GLES20.glCreateShader(type);
        GLES20.glShaderSource(shader, shaderCode);
        GLES20.glCompileShader(shader);
        return shader;
    }
}
```


```text
precision mediump float;
uniform vec4 vColor;
void main() {
 gl_FragColor = vColor;
}
```

```text
uniform mat4 u_Matrix;
attribute vec4 a_Position;
void main()
{
    gl_Position = u_Matrix * a_Position;
}
```



# openSLES
```text
OpenSL ES (Open Sound Library for Embedded Systems)
C 语言接口，兼容 C++ 。
是一个嵌入式、跨平台、免费的、音频处理库。

android NDK 已经内嵌，
位于类似 xxx\ndk-bundle\platforms\android-9\arch-x86\usr\lib

Android 2.3 (API 9) 开始支持 OpenSL ES 标准，
Android实现的 OpenSL ES 只是 OpenSL 1.0.1 的子集，并且进行了扩展。
```

##  OPenSL ES 常用功能
```c++
1、支持 PCM 数据的采集
支持的配置：16bit 位宽，16000 Hz采样率，单通道。（其他的配置不能保证兼容所有平台）

2、支持 PCM 数据的播放
支持的配置：8bit/16bit 位宽，单通道/双通道，小端模式，
采样率（8000, 11025, 12000, 16000, 22050, 24000, 32000, 44100, 48000 Hz）

3、支持播放的音频数据来源：
res 文件夹下的音频、
assets 文件夹下的音频、
sdcard 目录下的音频、
在线网络音频、
代码中定义的音频二进制数据等
```


##  OPenSL ES 的缺点
```c++
1、不支持版本低于 Android 2.3 (API 9) 的设备
2、没有全部实现 OpenSL ES 定义的特性和功能
3、不支持 MIDI
4、不支持直接播放 DRM 或者 加密的内容
5、不支持音频数据的编解码
```


##  OPenSL ES 状态机制
```c++
1、任何一个 OpenSL ES 的对象，创建成功后，都进入 SL_OBJECT_STATE_UNREALIZED 状态，
这种状态下，系统不会为它分配任何资源，直到调用 Realize 函数为止。

2、Realize 后的对象，就会进入 SL_OBJECT_STATE_REALIZED 状态，
这是一种“可用”的状态，只有在这种状态下，对象的各个功能和资源才能正常地访问。

3、当一些系统事件发生后，比如出现错误或者 Audio 设备被其他应用抢占，
OpenSL ES 对象会进入 SL_OBJECT_STATE_SUSPENDED 状态，
如果希望恢复正常使用，需要调用 Resume 函数。

4、当调用对象的 Destroy 函数后，则会释放资源，
并回到 SL_OBJECT_STATE_UNREALIZED 状态。

```

##  OPenSL ES 重要概念 ： Objects 和 Interfaces
```c++
1、每个 Object 可能会存在一个或者多个 Interface，
官方为每一种 Object 都定义了一系列的 Interface

2、每个 Object 对象都提供了一些最基础的操作 比如：Realize 、Destroy 等等，
但是想使用该对象支持的功能函数，则必须通过其 GetInterface 函数拿到 Interface 接口，
然后通过 Interface 来访问功能函数 。

3、心中保持一个概念，就是在 OpenSL ES 中，一切 API 的访问和控制都是通过 Interface 来完成的。
以引擎接口为例：
①、先通过 SLObjectItf 获得 引擎对象
②、再通过 引擎对象 获得 这个对象对应的引擎接口。
③、最后通过这个引擎接口实现具体功能。
```

<br>

## 常用的接口
在OpenSL ES中所有的操作都是通过接口来完成，和java的接口类似，接口提供底层的方法调用。

```c++
SLObjectItf ：对象接口
SLEngineItf ：引擎接口
SLPlayItf：播放接口
SLBufferQueueItf :缓冲队列接口
SLVolumeItf : 声量接口
```

### 引擎接口  SLEngineItf

```c++
// 引擎对象
SLObjectItf engineObject = NULL;
// 创建对象
SLresult result = slCreateEngine(&engineObject, 0, 0, 0, 0, 0);
//初始化对象
SLresult result2 = (*engineObject)->Realize(engineObject, SL_BOOLEAN_FALSE);
// 销毁对象
(*engineObject)->Destroy(engineObject);

// 引擎接口
SLEngineItf engineEngine = NULL;
// 获取对象的功能接口
SLresult result3 = (*engineObject)->GetInterface(engineObject, SL_IID_ENGINE, &engineEngine);
// 调用接口实现具体功能 ，例如 创建音频播放器对象
SLresult result4 = (*engineEngine)->CreateAudioPlayer(engineEngine, &pcmPlayerObject, &slDataSource, &audioSnk, 3, ids, req);
```

<br>
## 混音器
```c++
//混音器对象
SLObjectItf outputMixObject = NULL;
//创建混音器对象
SLresult result = (*engineEngine)->CreateOutputMix(engineEngine, &outputMixObject, 1, mids, mreq);
//实现混音器对象
result = (*outputMixObject)->Realize(outputMixObject, SL_BOOLEAN_FALSE);
// 跟进对象获取对应的功能接口
result = (*outputMixObject)->GetInterface(outputMixObject, SL_IID_ENVIRONMENTALREVERB, &outputMixEnvironmentalReverb);
```

### 常用函数
```text
SetPlayState () // 设置播放器状态
slCreateEngine () // 创建对象
Realize () //  分配资源
GetInterface () // 通过对象，拿到接口
CreateOutputMix () // 创建混音器
SetEnvironmentalReverbProperties () //
CreateAudioPlayer() // 创建播放器
```

### 常用类和常量
```text
SLEnvironmentalReverbItf       //  一个结构体 ，
SLEnvironmentalReverbSettings  // 一个结构体 ，
SLAndroidSimpleBufferQueueItf  // 队列
SLInterfaceID  //
SLDataLocator_OutputMix  //
SLDataSink //
SLDataSource // 

nb_samples // 每个轨道的样本数
channels   //音频通道数
channel_layout // 音频通道格式类型 ,如 单通道 双通道
sample_rate    // 采样率，Audio only. The number of bytes per coded audio frame
AV_SAMPLE_FMT_S16     //  一种音频格式
AV_CH_LAYOUT_STEREO   // 双声道
AV_CH_LAYOUT_SURROUND // 三声道

SL_DATALOCATOR_OUTPUTMIX //  0x00000004
SLDataLocator_AndroidSimpleBufferQueue  //
SL_DATALOCATOR_ANDROIDSIMPLEBUFFERQUEUE //
SLDataFormat_PCM // 
SL_DATAFORMAT_PCM //  pcm格式的数据
SL_PCMSAMPLEFORMAT_FIXED_16 //   位数16位
SL_SPEAKER_FRONT_LEFT       //  立体声,前左
SL_SPEAKER_FRONT_RIGHT     //立体声 ,前右
SL_BYTEORDER_LITTLEENDIAN   // 
SL_PLAYSTATE_PAUSED  // glsl的暂停状态常量
SL_PLAYSTATE_STOPPED //
SL_PLAYSTATE_PLAYING //
SL_IID_BUFFERQUEUE // 
SL_IID_PLAYBACKRATE //
SL_RESULT_SUCCESS  // 0
SL_BOOLEAN_FALSE  //false 
SL_BOOLEAN_TRUE //
SL_IID_ENGINE  // 一个常量，获取接口用到
SL_IID_ENVIRONMENTALREVERB // 
SL_IID_PLAY  //
SL_SAMPLINGRATE_8  // opensl中的采样率常量
SL_SAMPLINGRATE_11_025
SL_SAMPLINGRATE_12	
SL_SAMPLINGRATE_16
SL_SAMPLINGRATE_22_05
SL_SAMPLINGRATE_24
SL_SAMPLINGRATE_32
SL_SAMPLINGRATE_44_1
SL_SAMPLINGRATE_48
SL_SAMPLINGRATE_64
SL_SAMPLINGRATE_88_2
SL_SAMPLINGRATE_96
SL_SAMPLINGRATE_192
```

<br>
## 播放器
```c++
SLObjectItf pcmPlayerObject = NULL;
//创建对象
result = (*engineEngine)->CreateAudioPlayer(engineEngine, &pcmPlayerObject, &slDataSource, &audioSnk, 3, ids, req);
//实现对象
(*pcmPlayerObject)->Realize(pcmPlayerObject, SL_BOOLEAN_FALSE);
// 跟进对象获取 播放功能接口
SLPlayItf pcmPlayerPlay = NULL;
(*pcmPlayerObject)->GetInterface(pcmPlayerObject, SL_IID_PLAY, &pcmPlayerPlay);
// 使用功能接口
(*pcmPlayerPlay)->SetPlayState(pcmPlayerPlay,  SL_PLAYSTATE_PAUSED);
//获得 音量接口
SLVolumeItf pcmPlayerVolume = NULL;
(*pcmPlayerObject)->GetInterface(pcmPlayerObject, SL_IID_VOLUME, &pcmPlayerVolume);
```


<br>
## 使用 OPenSL ES 流程
```c++
1、创建接口对象
2、设置混音器
3、创建播放器（录音器）
4、设置缓冲队列和回调函数
5、设置播放状态
6、启动回调函数
```

##  Android 配置 opengSL ES
```text
1、cmakelist中引入  OpenSLES   

2、引入头文件
extern "C"
{
 include <SLES/OpenSLES.h>
include <SLES/OpenSLES_Android.h>
};
```


### 声道切换
```text
/*
   chan ： 0  左声道 ，1 右声道
   mute : true 开启声道
   立体声：左声道右声道都关闭
 */
 (*pcmMutePlay)->SetChannelMute(pcmMutePlay, 1, SL_BOOLEAN_FALSE);
 (*pcmMutePlay)->SetChannelMute(pcmMutePlay, 0, SL_BOOLEAN_TRUE);
```


### 暂停、播放
```text
(*pcmPlayerPlay)->SetPlayState(pcmPlayerPlay,  SL_PLAYSTATE_PAUSED);

(*pcmPlayerPlay)->SetPlayState(pcmPlayerPlay,  SL_PLAYSTATE_PLAYING);
```


### seek 修改播放进度
对直播无效
```text
int64_t rel = secds * AV_TIME_BASE;
avformat_seek_file(avFormatContext, -1, INT64_MIN, rel, INT64_MAX, 0);
```


### 修改音量
```text
 SLmillibel level =50;
(*pcmVolumePlay)->SetVolumeLevel(pcmVolumePlay, level);
```


### 边播边录
```text
利用 Android（api>=16）MediaCodec
将原始的PCM 数据 编码成 AAC 格式的数据 直接保存即可。
```


### 求PCM的分贝值
![](https://gitee.com/hnyer/filesOfGitbook/raw/master/files/201808151429_osChina_分贝计算公式.png)
```text
Pref：就是声音总的振幅最大值；
Prms：就是当前声音的振幅值；
Lp：就是我们需要的声音分贝值了。

根据公式我们可以得知把一段时间的振幅相加然后去平均值，就可以得到当前
时间段的振幅值，就可以求出分贝值了
```


# SoundTouch
```text
OpenSL ES可以实现变速播放，但是再改变速度的同时也改变了音调 。

SoundTouch  http://www.surina.net/soundtouch/
一个开源的声音处理库，可以直接对PCM数据进行处理，
可单独改变声音的音调和播放速度。
```

## 移植 SoundTouch
```text
1、下载  soundtouch-soundtouch-2.0.0.zip  
2、提取 头文件 和源码 放到 android工程中的cpp文件夹中
3、配置cmakelist

具体的文件和配置可以参考我的工程wkplayer
```

##  变速、变调 
```text
soundTouch->setPitch(pitch);//SoundTouch //变速 
soundTouch->setTempo(speed);// 变调
```
